{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import function_board as fb\n",
    "import function_tool as ft\n",
    "# import function_get_aiming_grid\n",
    "# import function_evaluate_policy as fep\n",
    "\n",
    "import init_simple_mdp as imdp\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(linewidth=300)\n",
    "np.set_printoptions(threshold=300)\n",
    "\n",
    "import torch\n",
    "torch.set_printoptions(precision=3)\n",
    "torch.set_printoptions(linewidth=300)\n",
    "torch.set_printoptions(threshold=300)\n",
    "\n",
    "import helpers as h\n",
    "\n",
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, './original_code/')\n",
    "\n",
    "import function_evaluate_policy as fep\n",
    "import function_get_aiming_grid as fgag\n",
    "import function_solve_dp as fsdp\n",
    "import helpers as h\n",
    "\n",
    "data_parameter_dir = fb.data_parameter_dir\n",
    "result_dir = './result'  \n",
    "name_pa = 'player{}'.format(10) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Baseline\n",
    "\n",
    "Run the best response algorithm for Haugh & Wang's version (no credits) as a baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_v2/player1_gaussin_prob_grid_v2.pkl\n",
      "runing solve_dp_turn\n",
      "solve prob_policy_transit in 0.2457590103149414 seconds\n",
      "solve dp_turn_policyiter in 1.8802990913391113 seconds\n",
      "[0.     0.     1.5506 ... 5.7062 5.6866 5.7317]\n",
      "dump_pickle to ./result/singlegame_player1_turn_v2.pkl\n",
      "\n",
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_v2/player2_gaussin_prob_grid_v2.pkl\n",
      "runing solve_dp_turn\n",
      "solve prob_policy_transit in 0.2570309638977051 seconds\n",
      "solve dp_turn_policyiter in 1.8312017917633057 seconds\n",
      "[0.     0.     1.9442 ... 5.7201 5.7249 5.7343]\n",
      "dump_pickle to ./result/singlegame_player2_turn_v2.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import function_board as fb\n",
    "import function_tool as ft\n",
    "from evaluate_score_probability_player import evaluate_score_probability\n",
    "import function_get_aiming_grid\n",
    "import function_solve_dp\n",
    "#import function_solve_zsg\n",
    "import function_solve_zsg_gpu as function_solve_zsg\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(linewidth=300)\n",
    "np.set_printoptions(threshold=300)\n",
    "np.seterr(divide='ignore')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parameter_dir = fb.data_parameter_dir\n",
    "result_dir = './result'        \n",
    "playerID_list = [1,2]\n",
    "gamelist = [[1,2]]\n",
    "\n",
    "# ## evaluate the hitting probability for each aiming location on the 1mm grid \n",
    "# evaluate_score_probability(playerID_list)\n",
    "\n",
    "# ## save the small action set which contains 984 aiming locations (grid_version='v2', used in the frist draft of the paper)\n",
    "# function_get_aiming_grid.save_aiming_grid_v2(playerID_list)\n",
    "\n",
    "\n",
    "# ## save the action set which contains 90,785 aiming locations (grid_version='circleboard', used in the current draft of the paper)\n",
    "# function_get_aiming_grid.save_aiming_grid_circleboard(playerID_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_player1_B_player2_optA: SB=25 DB=50 max_score=60 R1=6.35 grid_version_pa=v2 grid_version_pb=v2\n",
      "player A is player1 and player B is player2\n",
      "optimize player A policy and player B policy is fixed\n",
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_v2/player1_gaussin_prob_grid_v2.pkl\n",
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_v2/player2_gaussin_prob_grid_v2.pkl\n",
      "load player A policy ./result/singlegame_player1_turn_v2.pkl\n",
      "load player B policy ./result/singlegame_player2_turn_v2.pkl\n",
      "solve_zsg_opt_player1_fix_player2 in 482.06589007377625 seconds\n",
      "t_policy_evaluation  = 52.59022927284241 seconds\n",
      "t_policy_improvement = 423.54140758514404 seconds\n",
      "t_other = 0 seconds\n",
      "save ./result/zsg_A_player1_B_player2_optA.pkl\n",
      "save ./result/zsg_value_A_player1_B_player2_optA.pkl\n"
     ]
    }
   ],
   "source": [
    "## Choose the action set. 'v2' for testing purpose.\n",
    "grid_version = 'v2'\n",
    "#grid_version = 'circleboard'\n",
    "postfix=''\n",
    "\n",
    "gpu_device = None ## Using CPU\n",
    "#gpu_device = 0 ## Using GPU \n",
    "\n",
    "## solve the single player dart game\n",
    "for playerID in playerID_list:\n",
    "    name_pa = 'player{}'.format(playerID)   \n",
    "    res1 = function_solve_dp.solve_singlegame(name_pa, data_parameter_dir=data_parameter_dir, grid_version=grid_version, result_dir=result_dir, postfix=postfix, gpu_device=gpu_device)\n",
    "    print()\n",
    "\n",
    "# #%%\n",
    "# ## solve ZSG dart game between Player A and B\n",
    "# grid_version_pa = grid_version\n",
    "# grid_version_pb = grid_version\n",
    "# postfix='_{}'.format(grid_version)\n",
    "# for [pa, pb] in gamelist:\n",
    "#     name_pa = 'player{}'.format(pa)\n",
    "#     name_pb = 'player{}'.format(pb)\n",
    "        \n",
    "#     resA = function_solve_zsg.solve_zsg_optA_fixNS(name_pa, name_pb, data_parameter_dir=data_parameter_dir, grid_version_pa=grid_version_pa, grid_version_pb=grid_version_pb, dp_policy_folder=result_dir, result_dir=result_dir, postfix=postfix, gpu_device=gpu_device)\n",
    "#     print(time.asctime( time.localtime(time.time())))\n",
    "#     print()\n",
    "    \n",
    "#     if (pa!=pb):\n",
    "#         resB = function_solve_zsg.solve_zsg_optB_fixNS(name_pa, name_pb, data_parameter_dir=data_parameter_dir, grid_version_pa=grid_version_pa, grid_version_pb=grid_version_pb, dp_policy_folder=result_dir, result_dir=result_dir, postfix=postfix, gpu_device=gpu_device)\n",
    "#         print(time.asctime( time.localtime(time.time())))    \n",
    "#         print()\n",
    "    \n",
    "#     resboth = function_solve_zsg.solve_zsg_optboth(name_pa, name_pb, data_parameter_dir=data_parameter_dir, grid_version_pa=grid_version_pa, grid_version_pb=grid_version_pb, dp_policy_folder=result_dir, result_dir=result_dir, postfix=postfix, gpu_device=gpu_device)\n",
    "#     print(time.asctime(time.localtime(time.time())))\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import function_board as fb\n",
    "import function_tool as ft\n",
    "from evaluate_score_probability_player import evaluate_score_probability\n",
    "import function_get_aiming_grid\n",
    "import function_solve_dp\n",
    "#import function_solve_zsg\n",
    "import function_solve_zsg_gpu as function_solve_zsg\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(linewidth=300)\n",
    "np.set_printoptions(threshold=300)\n",
    "np.seterr(divide='ignore')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parameter_dir = fb.data_parameter_dir\n",
    "result_dir = './result'        \n",
    "playerID_list = [1,2]\n",
    "gamelist = [[1,2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose the action set. 'v2' for testing purpose.\n",
    "grid_version = 'v2'\n",
    "# grid_version = 'circleboard'\n",
    "postfix=''\n",
    "grid_version_pa = grid_version\n",
    "grid_version_pb = grid_version\n",
    "gpu_device = None ## Using CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_custom_no_tokens/player10_e1_gaussin_prob_grid_custom_no_tokens.pkl\n",
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_custom_tokens/t_gaussin_prob_grid_custom_tokens.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachael/Desktop/darts-thesis/helpers.py:719: RuntimeWarning: divide by zero encountered in divide\n",
      "  num_tothrow = num_tothrow / prob_otherstate\n",
      "/Users/rachael/Desktop/darts-thesis/helpers.py:719: RuntimeWarning: overflow encountered in divide\n",
      "  num_tothrow = num_tothrow / prob_otherstate\n",
      "/Users/rachael/Desktop/darts-thesis/helpers.py:757: RuntimeWarning: divide by zero encountered in divide\n",
      "  num_tothrow = num_tothrow / prob_otherstate\n",
      "/Users/rachael/Desktop/darts-thesis/helpers.py:757: RuntimeWarning: overflow encountered in divide\n",
      "  num_tothrow = num_tothrow / prob_otherstate\n",
      "/Users/rachael/Desktop/darts-thesis/helpers.py:1176: RuntimeWarning: divide by zero encountered in divide\n",
      "  num_turns_array[:,score_gained_index] = num_turns_array[:,score_gained_index] / prob_notbust_dic_nt[score_max]\n",
      "/Users/rachael/Desktop/darts-thesis/helpers.py:1176: RuntimeWarning: overflow encountered in divide\n",
      "  num_turns_array[:,score_gained_index] = num_turns_array[:,score_gained_index] / prob_notbust_dic_nt[score_max]\n",
      "/Users/rachael/Desktop/darts-thesis/helpers.py:1163: RuntimeWarning: divide by zero encountered in divide\n",
      "  num_turns_array[:imdp.throw_num,score_gained_index] = num_turns_array[:imdp.throw_num,score_gained_index] / prob_notbust_dic_nt[score_max][:imdp.throw_num]\n",
      "/Users/rachael/Desktop/darts-thesis/helpers.py:1163: RuntimeWarning: overflow encountered in divide\n",
      "  num_turns_array[:imdp.throw_num,score_gained_index] = num_turns_array[:imdp.throw_num,score_gained_index] / prob_notbust_dic_nt[score_max][:imdp.throw_num]\n",
      "/Users/rachael/Desktop/darts-thesis/helpers.py:1169: RuntimeWarning: divide by zero encountered in divide\n",
      "  num_turns_array[imdp.throw_num:,score_gained_index] = num_turns_array[imdp.throw_num:,score_gained_index] / prob_notbust_dic_t[score_max][imdp.throw_num:]\n",
      "/Users/rachael/Desktop/darts-thesis/helpers.py:1196: RuntimeWarning: divide by zero encountered in divide\n",
      "  value_relerror[rt] = np.abs((state_value_update[rt] - state_value[rt])/state_value_update[rt]).max()\n",
      "/Users/rachael/Desktop/darts-thesis/helpers.py:1196: RuntimeWarning: invalid value encountered in divide\n",
      "  value_relerror[rt] = np.abs((state_value_update[rt] - state_value[rt])/state_value_update[rt]).max()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solve prob_policy_transit in 2.756683826446533 seconds\n",
      "solve dp_turn_policyiter in 27.291189908981323 seconds\n",
      "[[0.     0.     1.8856 ... 5.5944 5.5758 5.6116]\n",
      " [0.     0.     1.     ... 4.9684 4.9631 4.9765]\n",
      " [0.     0.     1.     ... 4.7444 4.7403 4.7516]\n",
      " ...\n",
      " [0.     0.     1.     ... 3.8073 3.8027 3.8082]\n",
      " [0.     0.     1.     ... 3.5561 3.5447 3.5554]\n",
      " [0.     0.     1.     ... 3.     3.     3.    ]]\n"
     ]
    }
   ],
   "source": [
    "# Baseline \n",
    "name_pa = 'player{}'.format(1)\n",
    "name_pb = 'player{}'.format(2)\n",
    "\n",
    "resA = function_solve_zsg.solve_zsg_optA_fixNS(name_pa, name_pb, data_parameter_dir=data_parameter_dir, grid_version_pa=grid_version_pa, grid_version_pb=grid_version_pb, dp_policy_folder=result_dir, result_dir=result_dir, postfix=postfix, gpu_device=gpu_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.0 Policy Evaluation [DONE]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "def zsg_policy_evaluation_tokens(value_pa, value_pb, tokens_pa, score_state_pa, score_state_pb, prob_turn_transit_pa, prob_turn_transit_pb):\n",
    "    \"\"\"\n",
    "    Compute the game value (in terms of Player A's winning probability) for a specific turn (score_state_pa, score_state_pb) given Player A and B's policies. \n",
    "    Args: \n",
    "        value_pa, value_pb: game values for [s_a < score_state_pa, s_b < score_state_pb] are already solved. \n",
    "        score_state_pa, score_state_pb: scores for Player A and B at the beginning of this turn\n",
    "        prob_turn_transit_pa, prob_turn_transit_pb: state transition probability associated with the given policies. \n",
    "    \n",
    "    Returns: \n",
    "        value_state_pa: Player A's winning probability when A throws in this turn\n",
    "        value_state_pb: Player A's winning probability when B throws in this turn\n",
    "    \"\"\"\n",
    "    \n",
    "    value_win_pa = 1 # A win\n",
    "    value_win_pb = 0 # A lose\n",
    "\n",
    "    score_max_pa = min(score_state_pa-2, 3*fb.maxhitscore)\n",
    "    score_max_pb = min(score_state_pb-2, 3*fb.maxhitscore)\n",
    "\n",
    "    prob_score_pa = prob_turn_transit_pa['score']\n",
    "    prob_score_pb = prob_turn_transit_pb['score']\n",
    "    prob_finish_pa = prob_turn_transit_pa['finish']\n",
    "    prob_finish_pb = prob_turn_transit_pb['finish']\n",
    "    prob_zeroscore_pa = prob_turn_transit_pa['bust'][0] + prob_score_pa[0][0]\n",
    "    prob_zeroscore_pb = prob_turn_transit_pb['bust'][0] + prob_score_pb[0][0]\n",
    "\n",
    "    possible_tokens_used = prob_turn_transit_pa['bust'].shape[0] - 1\n",
    "\n",
    "    # sa = sum(sum(prob_turn_transit_pa['score'])) + sum(prob_turn_transit_pa['bust']) + prob_turn_transit_pa['finish']\n",
    "    # sb = sum(sum(prob_turn_transit_pb['score'])) + sum(prob_turn_transit_pb['bust']) + prob_turn_transit_pb['finish']\n",
    "    # print(sa,sb)\n",
    "\n",
    "    # Transit to end\n",
    "    constant_pa = prob_finish_pa * value_win_pa\n",
    "\n",
    "    # Only throws startying from Player A's turn, score greater than zero\n",
    "    constant_pa += np.dot(prob_score_pa[0,1:], value_pb[tokens_pa, score_state_pa-1:score_state_pa-score_max_pa-1:-1, score_state_pb])        \n",
    "\n",
    "    # Use at least one token:\n",
    "    if tokens_pa > 0:\n",
    "\n",
    "        # Go through subsequent scores after using at least one token \n",
    "        for i in range(0,possible_tokens_used):\n",
    "\n",
    "            constant_pa += np.dot(np.flip(prob_score_pa[0:possible_tokens_used+1,:],axis=0)[i], value_pb[tokens_pa-possible_tokens_used:tokens_pa+1,score_state_pa:score_state_pa-score_max_pa-1:-1, score_state_pb][i])        \n",
    "\n",
    "        # Go bust after using at least one token \n",
    "        constant_pa += np.dot(np.flip(prob_turn_transit_pa['bust'][1:possible_tokens_used+1]) , value_pb[tokens_pa-possible_tokens_used+1:tokens_pa+1,score_state_pa,score_state_pb])\n",
    "        \n",
    "    constant_pb = prob_finish_pb * value_win_pb #0\n",
    "    constant_pb += np.dot(prob_score_pb[0][1:], value_pa[tokens_pa,score_state_pa, score_state_pb-1:score_state_pb-score_max_pb-1:-1])\n",
    "\n",
    "    value_state_pa = (constant_pa+constant_pb*prob_zeroscore_pa)/(1-prob_zeroscore_pa*prob_zeroscore_pb)\n",
    "    value_state_pb = constant_pb + value_state_pa*prob_zeroscore_pb\n",
    "    \n",
    "    return [value_state_pa, value_state_pb]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ns_policy_dicts(name_pw,name_ps,epsilon_pw,epsilon_ps,dp_policy_folder,aiming_grid, prob_grid_normalscore_nt_pw, prob_grid_singlescore_nt_pw, prob_grid_doublescore_nt_pw, prob_grid_triplescore_nt_pw, prob_grid_bullscore_nt_pw, prob_grid_normalscore_nt_ps, prob_grid_singlescore_nt_ps, prob_grid_doublescore_nt_ps, prob_grid_triplescore_nt_ps, prob_grid_bullscore_nt_ps, prob_grid_normalscore_t, prob_grid_singlescore_t, prob_grid_doublescore_t, prob_grid_triplescore_t, prob_grid_bullscore_t):\n",
    "     ## use single player game as the fixed policy    \n",
    "    dp_policy_dict_pw = None\n",
    "    dp_policy_dict_ps = None\n",
    "    if dp_policy_folder is not None:\n",
    "        dp_policy_filename_pw = dp_policy_folder + '/singlegame_{}_e{}_turn_tokens.pkl'.format(name_pw,epsilon_pw)\n",
    "        if (os.path.isfile(dp_policy_filename_pw) == True):\n",
    "            dp_policy_dict_pw = ft.load_pickle(dp_policy_filename_pw)\n",
    "            print('load weaker player policy {}'.format(dp_policy_filename_pw))\n",
    "        dp_policy_filename_ps = dp_policy_folder + '/singlegame_{}_e{}_turn_tokens.pkl'.format(name_ps,epsilon_ps)\n",
    "        if (os.path.isfile(dp_policy_filename_ps) == True):\n",
    "            dp_policy_dict_ps = ft.load_pickle(dp_policy_filename_ps)\n",
    "            print('load stronger player policy {}'.format(dp_policy_filename_ps))    \n",
    "    if dp_policy_dict_pw is None:\n",
    "        print('solve weaker player NS policy')\n",
    "        dp_policy_dict_pw = h.solve_dp_turn_tokens(9, aiming_grid, prob_grid_normalscore_nt_pw, prob_grid_singlescore_nt_pw, prob_grid_doublescore_nt_pw, prob_grid_triplescore_nt_pw, prob_grid_bullscore_nt_pw, prob_grid_normalscore_t, prob_grid_singlescore_t, prob_grid_doublescore_t, prob_grid_triplescore_t, prob_grid_bullscore_t)\n",
    "    if dp_policy_dict_ps is None:\n",
    "        print('solve stronger player NS policy')\n",
    "        dp_policy_dict_ps = h.solve_dp_turn_tokens(9, aiming_grid, prob_grid_normalscore_nt_ps, prob_grid_singlescore_nt_ps, prob_grid_doublescore_nt_ps, prob_grid_triplescore_nt_ps, prob_grid_bullscore_nt_ps, prob_grid_normalscore_t, prob_grid_singlescore_t, prob_grid_doublescore_t, prob_grid_triplescore_t, prob_grid_bullscore_t)\n",
    "\n",
    "    return dp_policy_dict_pw,dp_policy_dict_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "player_num = 10 \n",
    "epsilon = 1\n",
    "tokens = 9\n",
    "name_pa = 'player{}'.format(player_num)\n",
    "\n",
    "[aiming_grid, prob_grid_normalscore_nt, prob_grid_singlescore_nt, prob_grid_doublescore_nt, prob_grid_triplescore_nt, prob_grid_bullscore_nt] = h.load_aiming_grid(name_pa, epsilon=epsilon, data_parameter_dir=data_parameter_dir, grid_version='custom_no_tokens')\n",
    "[aiming_grid, prob_grid_normalscore_t, prob_grid_singlescore_t, prob_grid_doublescore_t, prob_grid_triplescore_t, prob_grid_bullscore_t] = h.load_aiming_grid('t', data_parameter_dir=data_parameter_dir, grid_version='custom_tokens')\n",
    "\n",
    "prob_grid_doublescore_dic = None\n",
    "prob_grid_doublescore_dic_t = None\n",
    "\n",
    "result_dic = h.solve_dp_turn_tokens(tokens, aiming_grid, prob_grid_normalscore_nt, prob_grid_singlescore_nt, prob_grid_doublescore_nt, prob_grid_triplescore_nt, prob_grid_bullscore_nt,prob_grid_normalscore_t, prob_grid_singlescore_t, prob_grid_doublescore_t, prob_grid_triplescore_t, prob_grid_bullscore_t)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_player10e2_S_player10e1_optW\n",
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_custom_no_tokens/player10_e2_gaussin_prob_grid_custom_no_tokens.pkl\n",
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_custom_no_tokens/player10_e1_gaussin_prob_grid_custom_no_tokens.pkl\n",
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_custom_tokens/t_gaussin_prob_grid_custom_tokens.pkl\n",
      "load weaker player policy result/singlegame_player10_e2_turn_tokens.pkl\n",
      "load stronger player policy result/singlegame_player10_e1_turn_tokens.pkl\n",
      "stronger state: 2 time: 0.0002989768981933594\n",
      "stronger state: 3 time: 2.370417833328247\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[122], line 154\u001B[0m\n\u001B[1;32m    152\u001B[0m tpi1 \u001B[39m=\u001B[39m time\u001B[39m.\u001B[39mtime()\n\u001B[1;32m    153\u001B[0m param_pw[\u001B[39m'\u001B[39m\u001B[39mround_index\u001B[39m\u001B[39m'\u001B[39m] \u001B[39m=\u001B[39m round_index\n\u001B[0;32m--> 154\u001B[0m [max_action_diff, max_value_relerror] \u001B[39m=\u001B[39m zsg_policy_improvement_tokens(param_pw)\n\u001B[1;32m    155\u001B[0m tpi2 \u001B[39m=\u001B[39m time\u001B[39m.\u001B[39mtime()\n\u001B[1;32m    156\u001B[0m t_policy_improvement \u001B[39m+\u001B[39m\u001B[39m=\u001B[39m (tpi2 \u001B[39m-\u001B[39m tpi1)                \n",
      "Cell \u001B[0;32mIn[50], line 142\u001B[0m, in \u001B[0;36mzsg_policy_improvement_tokens\u001B[0;34m(param)\u001B[0m\n\u001B[1;32m    137\u001B[0m     win_prob_tensor[imdp\u001B[39m.\u001B[39mthrow_num:\u001B[39mlen\u001B[39m(aiming_grid)] \u001B[39m+\u001B[39m\u001B[39m=\u001B[39m prob_normalscore_tensor_t[imdp\u001B[39m.\u001B[39mthrow_num:]\u001B[39m.\u001B[39mmatmul(next_state_value_tensor_t)\n\u001B[1;32m    139\u001B[0m \u001B[39m# if we don't have tokens   \u001B[39;00m\n\u001B[1;32m    140\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m    141\u001B[0m     \u001B[39m# add no token expectation --> index is 1 because we keep the same # of tokens       \u001B[39;00m\n\u001B[0;32m--> 142\u001B[0m     win_prob_tensor \u001B[39m=\u001B[39m prob_normalscore_tensor_nt\u001B[39m.\u001B[39;49mmatmul(next_state_value_tensor_nt)\n\u001B[1;32m    144\u001B[0m \u001B[39m## searching\u001B[39;00m\n\u001B[1;32m    145\u001B[0m \u001B[39mif\u001B[39;00m flag_max:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "print(result_dic.keys())\n",
    "result_dic['prob_scorestate_transit'][0]#.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.0 Policy Improvement"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zsg_policy_improvement_tokens(param):\n",
    "    \"\"\"\n",
    "    Do a policy improvement. Solve the Bellman equation. Find the best aiming location for each state in a turn using the last updated state values. \n",
    "    Args: \n",
    "        a dict param containing necessary informations. \n",
    "    \n",
    "    Returns:     \n",
    "        max_action_diff, max_value_relerror: relative errors after this policy iteration step \n",
    "    \"\"\"\n",
    "    #### input value ####\n",
    "    prob_normalscore_tensor_nt = param['prob_normalscore_tensor_nt']\n",
    "    prob_normalscore_tensor_t = param['prob_normalscore_tensor_t']\n",
    "    prob_doublescore_dic_nt = param['prob_doublescore_dic_nt']\n",
    "    prob_doublescore_dic_t = param['prob_doublescore_dic_t']\n",
    "    prob_DB_nt = param['prob_DB_nt']\n",
    "    prob_DB_t = param['prob_DB_t']\n",
    "    prob_bust_dic_nt = param['prob_bust_dic_nt']\n",
    "    prob_bust_dic_t = param['prob_bust_dic_t']\n",
    "    num_actions = prob_normalscore_tensor_nt.shape[0]\n",
    "\n",
    "    state_len_vector = param['state_len_vector']\n",
    "    score_state = param['score_state']   \n",
    "    token_state = param['token_state']  \n",
    "    state_action = param['state_action']\n",
    "    state_value = param['state_value']         \n",
    "    state_action_update = param['state_action_update']\n",
    "    state_value_update = param['state_value_update']\n",
    "    action_diff = param['action_diff']\n",
    "    value_relerror = param['value_relerror']\n",
    "\n",
    "    flag_max = param['flag_max'] ## maximize or minimize \n",
    "    next_turn_value = param['next_turn_value']\n",
    "    game_end_value = param['game_end_value']    \n",
    "    if 'round_index' in param:\n",
    "        round_index = param['round_index']\n",
    "    else:\n",
    "        round_index = 0\n",
    "\n",
    "    #### policy improvement ####\n",
    "    for rt in [1,2,3]:\n",
    "\n",
    "        this_throw_state_len = state_len_vector[rt]\n",
    "        state_notbust_len =  max(min(score_state-61, this_throw_state_len),0)\n",
    "        token_index = min(2,token_state+1)\n",
    "\n",
    "        ## CASE 1: state which can not bust.  score_state-score_gained>=62 \n",
    "        if (state_notbust_len > 0):\n",
    "            \n",
    "            # One throw remaining and first round of iteration \n",
    "            if (rt==1 and round_index==0):\n",
    "                ## combine all non-bust states together \n",
    "                state_notbust_update_index = state_notbust_len   \n",
    "                next_state_value_array_nt = np.zeros((61, state_notbust_len))   \n",
    "                next_state_value_array_t = np.zeros((61, state_notbust_len))                    \n",
    "                for score_gained in range(state_notbust_len):\n",
    "                    ## skip infeasible state\n",
    "                    if not fb.state_feasible_array[rt, score_gained]:\n",
    "                        continue\n",
    "                    score_remain = score_state - score_gained\n",
    "                    score_max = 60 ## always 60 here\n",
    "                    score_max_plus1 = score_max + 1\n",
    "\n",
    "                    new_state_vals_nt = next_turn_value[token_state]\n",
    "                    next_state_value_array_nt[:,score_gained] = new_state_vals_nt[score_remain:score_remain-score_max_plus1:-1]\n",
    "\n",
    "                    if token_state > 0: \n",
    "                        new_state_vals_t = next_turn_value[token_state-1]\n",
    "                        next_state_value_array_t[:,score_gained] = new_state_vals_t[score_remain:score_remain-score_max_plus1:-1]\n",
    "\n",
    "            # Two throws remaining and first round of iteration \n",
    "            elif (rt==2 and (round_index==0 or score_state<182)):\n",
    "                ## combine all non-bust states together \n",
    "                state_notbust_update_index = state_notbust_len\n",
    "                next_state_value_array_nt = np.zeros((61, state_notbust_len))      \n",
    "                next_state_value_array_t = np.zeros((61, state_notbust_len))                        \n",
    "                for score_gained in range(state_notbust_len):\n",
    "                    ## skip infeasible state\n",
    "                    if not fb.state_feasible_array[rt, score_gained]:\n",
    "                        continue\n",
    "                    score_remain = score_state - score_gained\n",
    "                    score_max = 60 ## always 60 here\n",
    "                    score_max_plus1 = score_max + 1\n",
    "                    \n",
    "                    new_state_vals_nt = state_value_update[rt-1][token_state]\n",
    "                    next_state_value_array_nt[:,score_gained] = new_state_vals_nt[score_gained:score_gained+score_max_plus1]\n",
    "\n",
    "                    if token_state > 0: \n",
    "                        new_state_vals_t = state_value_update[rt-1][token_state-1]\n",
    "                        next_state_value_array_t[:,score_gained] = new_state_vals_t[score_gained:score_gained+score_max_plus1]\n",
    "            \n",
    "            # Three throws remaining \n",
    "            # OR two throws remaining past first round of iteration and score state is greater than 182 \n",
    "            # OR one throw remaining past first round of iteration  \n",
    "            else: ##(rt==1 and round_index>0) or (rt==2 and round_index>0 and score_state>=182) or (rt==3)\n",
    "                ## only update state of score_gained = 0\n",
    "                state_notbust_update_index = 1\n",
    "                next_state_value_array_nt= np.zeros((61))\n",
    "                next_state_value_array_t= np.zeros((61))\n",
    "                score_gained = 0\n",
    "                score_remain = score_state - score_gained\n",
    "                score_max = 60 ## always 60 here\n",
    "                score_max_plus1 = score_max + 1                    \n",
    "                ## make a copy\n",
    "                if (rt > 1):\n",
    "\n",
    "                    new_state_vals_nt = state_value_update[rt-1][token_state]\n",
    "                    next_state_value_array_nt[:] = new_state_vals_nt[score_gained:score_gained+score_max_plus1]\n",
    "                    \n",
    "                    if token_state > 0:\n",
    "                        new_state_vals_t = state_value_update[rt-1][token_state-1]\n",
    "                        next_state_value_array_t[:] = new_state_vals_t[score_gained:score_gained+score_max_plus1]\n",
    "                \n",
    "                ## transit to next turn when rt=1\n",
    "                else:\n",
    "\n",
    "                    new_state_vals_nt = next_turn_value[token_state]\n",
    "                    next_state_value_array_nt[:] = new_state_vals_nt[score_remain:score_remain-score_max_plus1:-1]\n",
    "                    \n",
    "                    if token_state > 0: \n",
    "                        new_state_vals_t = next_turn_value[token_state-1]\n",
    "                        next_state_value_array_t[:] = new_state_vals_t[score_remain:score_remain-score_max_plus1:-1]\n",
    "                        \n",
    "            ## matrix product to compute all together\n",
    "            next_state_value_tensor_nt = torch.from_numpy(next_state_value_array_nt)\n",
    "            next_state_value_tensor_t = torch.from_numpy(next_state_value_array_t)\n",
    "\n",
    "            try: \n",
    "                win_prob_tensor = torch.zeros((num_actions,next_state_value_array_nt.shape[1]))\n",
    "            except:\n",
    "                win_prob_tensor = torch.zeros(num_actions)\n",
    "            \n",
    "            # if we have tokens \n",
    "            if token_state > 0:  \n",
    "\n",
    "                # use no token probabilities for no token actions \n",
    "                win_prob_tensor[:imdp.throw_num] += prob_normalscore_tensor_nt[:imdp.throw_num].matmul(next_state_value_tensor_nt)\n",
    "                # use token probabilities for token actions \n",
    "                win_prob_tensor[imdp.throw_num:num_actions] += prob_normalscore_tensor_t[imdp.throw_num:].matmul(next_state_value_tensor_t)\n",
    "\n",
    "            # if we don't have tokens   \n",
    "            else:\n",
    "                # add no token expectation --> index is 1 because we keep the same # of tokens       \n",
    "                win_prob_tensor = prob_normalscore_tensor_nt.matmul(next_state_value_tensor_nt)\n",
    "\n",
    "            ## searching\n",
    "            if flag_max:\n",
    "                if token_state == 0: \n",
    "                    temp1 = win_prob_tensor[:imdp.throw_num].max(axis=0)\n",
    "                else: \n",
    "                    temp1 = win_prob_tensor.max(axis=0) \n",
    "            else:\n",
    "                if token_state == 0: \n",
    "                    temp1 = win_prob_tensor[:imdp.throw_num].min(axis=0)\n",
    "                else: \n",
    "                    temp1 = win_prob_tensor.min(axis=0) \n",
    "\n",
    "            state_action_update[rt][token_state][0:state_notbust_update_index] = temp1.indices.numpy()\n",
    "            state_value_update[rt][token_state][0:state_notbust_update_index] =  temp1.values.numpy()                \n",
    "        \n",
    "        ## CASE 2: state which possibly bust.  score_state-score_gained<62 \n",
    "        if (state_notbust_len < this_throw_state_len):\n",
    "            ## combine all bust states together \n",
    "            state_bust_len = this_throw_state_len - state_notbust_len\n",
    "            next_state_value_array_nt = np.zeros((61, state_bust_len))\n",
    "            next_state_value_array_t = np.zeros((61, state_bust_len))     \n",
    "            for score_gained in range(state_notbust_len, this_throw_state_len):\n",
    "                ## skip infeasible state\n",
    "                if not fb.state_feasible_array[rt, score_gained]:\n",
    "                    continue\n",
    "                score_remain = score_state - score_gained\n",
    "                #score_max = min(score_remain-2, 60)\n",
    "                score_max = score_remain-2 ## less than 60 here\n",
    "                score_max_plus1 = score_max + 1\n",
    "                score_gained_index = score_gained - state_notbust_len ## index off set\n",
    "                if (rt > 1):\n",
    "                    \n",
    "                    new_state_vals_nt = state_value_update[rt-1][token_state]\n",
    "                    next_state_value_array_nt[0:score_max_plus1,score_gained_index] = new_state_vals_nt[score_gained:score_gained+score_max_plus1]\n",
    "\n",
    "                    if token_state > 0: \n",
    "                        new_state_vals_t = state_value_update[rt-1][token_state-1]\n",
    "                        next_state_value_array_t[0:score_max_plus1,score_gained_index] = new_state_vals_t[score_gained:score_gained+score_max_plus1]\n",
    "\n",
    "                ## transit to next turn when rt=1\n",
    "                else:\n",
    "\n",
    "                    new_state_vals_nt = next_turn_value[token_state]\n",
    "                    next_state_value_array_nt[0:score_max_plus1,score_gained_index] = new_state_vals_nt[score_remain:score_remain-score_max_plus1:-1]\n",
    "\n",
    "                    \n",
    "                    if token_state > 0: \n",
    "                        new_state_vals_t = next_turn_value[token_state-1]\n",
    "                        next_state_value_array_t[0:score_max_plus1,score_gained_index] = new_state_vals_t[score_remain:score_remain-score_max_plus1:-1]\n",
    "                        \n",
    "            ## matrix product to compute all together\n",
    "            next_state_value_tensor_nt = torch.from_numpy(next_state_value_array_nt)\n",
    "            next_state_value_tensor_t = torch.from_numpy(next_state_value_array_t)\n",
    "\n",
    "            # initialized\n",
    "            try: \n",
    "                win_prob_tensor = torch.zeros((num_actions,next_state_value_array_nt.shape[1]))\n",
    "            except:\n",
    "                win_prob_tensor = torch.zeros(num_actions)\n",
    "        \n",
    "            # if we have tokens \n",
    "            if token_state > 0: \n",
    "                # use no token probabilities for no token actions\n",
    "                win_prob_tensor[:imdp.throw_num] += prob_normalscore_tensor_nt[:imdp.throw_num].matmul(next_state_value_tensor_nt)\n",
    "                \n",
    "                # use token probabilities for token actions \n",
    "                win_prob_tensor[imdp.throw_num:] += prob_normalscore_tensor_t[imdp.throw_num:].matmul(next_state_value_tensor_t)\n",
    "\n",
    "            else: \n",
    "                # add no token expectation  \n",
    "                win_prob_tensor += prob_normalscore_tensor_nt.matmul(next_state_value_tensor_nt)\n",
    "\n",
    "            ## consider bust/finishing for each bust state seperately \n",
    "            win_prob_array = win_prob_tensor.numpy()                \n",
    "            for score_gained in range(state_notbust_len, this_throw_state_len):\n",
    "                ## skip infeasible state\n",
    "                if not fb.state_feasible_array[rt, score_gained]:\n",
    "                    continue\n",
    "                score_remain = score_state - score_gained\n",
    "                #score_max = min(score_remain-2, 60)\n",
    "                score_max = score_remain-2 ## less than 60 here\n",
    "                score_max_plus1 = score_max + 1\n",
    "                score_gained_index = score_gained - state_notbust_len\n",
    "\n",
    "                ## transit to the end of game\n",
    "                if (score_remain == fb.score_DB):                        \n",
    "                    win_prob_array[:imdp.throw_num,score_gained_index] += prob_DB_nt[:imdp.throw_num]*game_end_value\n",
    "                    win_prob_array[imdp.throw_num:,score_gained_index] += prob_DB_t[imdp.throw_num:]*game_end_value\n",
    "                elif (score_remain <= 40 and score_remain%2==0):\n",
    "                    win_prob_array[:imdp.throw_num,score_gained_index] += prob_doublescore_dic_nt[score_remain][:imdp.throw_num]*game_end_value\n",
    "                    win_prob_array[imdp.throw_num:,score_gained_index] += prob_doublescore_dic_t[score_remain][imdp.throw_num:]*game_end_value\n",
    "                else:\n",
    "                    pass  \n",
    "\n",
    "                ## transit to bust\n",
    "                win_prob_array[:imdp.throw_num,score_gained_index] += prob_bust_dic_nt[score_max][:imdp.throw_num]*(next_turn_value[token_state][score_state])  ## 1 turn is already counted before\n",
    "                win_prob_array[imdp.throw_num:,score_gained_index] += prob_bust_dic_t[score_max][imdp.throw_num:]*(next_turn_value[token_state-1][score_state])\n",
    "                    \n",
    "            ## searching\n",
    "            if flag_max:\n",
    "                if token_state == 0: \n",
    "                    temp1 = win_prob_tensor[:imdp.throw_num].max(axis=0)\n",
    "                else: \n",
    "                    temp1 = win_prob_tensor.max(axis=0)\n",
    "            else:\n",
    "                if token_state == 0: \n",
    "                    temp1 = win_prob_tensor[:imdp.throw_num].min(axis=0)\n",
    "                else: \n",
    "                    temp1 = win_prob_tensor.min(axis=0)\n",
    "            state_action_update[rt][token_state][state_notbust_len:this_throw_state_len] = temp1.indices.numpy()\n",
    "            state_value_update[rt][token_state][state_notbust_len:this_throw_state_len] =  temp1.values.numpy()                \n",
    "\n",
    "        #### finish rt=1,2,3. check improvement\n",
    "        action_diff[rt][token_state][:] = np.abs(state_action_update[rt][token_state] - state_action[rt][token_state])                                \n",
    "        value_relerror[rt] = np.abs((state_value_update[rt] - state_value[rt])/state_value_update[rt]).max()\n",
    "        state_action[rt][token_state][:] = state_action_update[rt][token_state][:]\n",
    "        state_value[rt][token_state][:] = state_value_update[rt][token_state][:]\n",
    "\n",
    "    max_action_diff = max([action_diff[1].max(), action_diff[2].max(), action_diff[3].max()])\n",
    "    max_value_relerror = value_relerror.max()\n",
    "\n",
    "    return [max_action_diff, max_value_relerror]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Solve BR for PW \n",
    "\n",
    "Solve the best response for the weaker player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "playerID = 10\n",
    "name_pw = 'player{}'.format(playerID) \n",
    "name_ps = 'player{}'.format(playerID) \n",
    "epsilon_pw = 2\n",
    "epsilon_ps = 1\n",
    "dp_policy_folder = 'result'\n",
    "postfix=''\n",
    "gpu_device=None\n",
    "\n",
    "\n",
    "info = 'W_{}e{}_S_{}e{}_optW'.format(name_pw, epsilon_pw, name_ps, epsilon_ps)\n",
    "print(info)\n",
    "max_tokens=0\n",
    "\n",
    "##\n",
    "if result_dir is not None:\n",
    "    if not os.path.isdir(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "    result_filename = result_dir + '/zsg_W_{}e{}_S_{}e{}_{}_optW.pkl'.format(name_pw, epsilon_pw, name_ps, epsilon_ps, postfix)\n",
    "    result_value_filename = result_dir + '/zsg_value_W_{}e{}_S_{}e{}_{}_optW.pkl'.format(name_pw, epsilon_pw, name_ps, epsilon_ps, postfix)\n",
    "\n",
    "\n",
    "[aiming_grid, prob_grid_normalscore_nt_pw, prob_grid_singlescore_nt_pw, prob_grid_doublescore_nt_pw, prob_grid_triplescore_nt_pw, prob_grid_bullscore_nt_pw] = h.load_aiming_grid(name_pw, epsilon=epsilon_pw, data_parameter_dir=data_parameter_dir, grid_version='custom_no_tokens')\n",
    "[aiming_grid, prob_grid_normalscore_nt_ps, prob_grid_singlescore_nt_ps, prob_grid_doublescore_nt_ps, prob_grid_triplescore_nt_ps, prob_grid_bullscore_nt_ps] = h.load_aiming_grid(name_ps, epsilon=epsilon_ps, data_parameter_dir=data_parameter_dir, grid_version='custom_no_tokens')\n",
    "[aiming_grid, prob_grid_normalscore_t, prob_grid_singlescore_t, prob_grid_doublescore_t, prob_grid_triplescore_t, prob_grid_bullscore_t] = h.load_aiming_grid('t', data_parameter_dir=data_parameter_dir, grid_version='custom_tokens')\n",
    "\n",
    "## use single player game as the fixed policy    \n",
    "dp_policy_dict_pw, dp_policy_dict_ps = load_ns_policy_dicts(name_pw,name_ps,epsilon_pw,epsilon_ps,dp_policy_folder,aiming_grid, prob_grid_normalscore_nt_pw, prob_grid_singlescore_nt_pw, prob_grid_doublescore_nt_pw, prob_grid_triplescore_nt_pw, prob_grid_bullscore_nt_pw, prob_grid_normalscore_nt_ps, prob_grid_singlescore_nt_ps, prob_grid_doublescore_nt_ps, prob_grid_triplescore_nt_ps, prob_grid_bullscore_nt_ps, prob_grid_normalscore_t, prob_grid_singlescore_t, prob_grid_doublescore_t, prob_grid_triplescore_t, prob_grid_bullscore_t)\n",
    "\n",
    "\n",
    "num_aiming_location_pw, prob_normalscore_nt_pw, prob_doublescore_dic_nt_pw, prob_DB_nt_pw, prob_bust_dic_nt_pw, prob_notbust_dic_nt_pw, prob_normalscore_t, prob_doublescore_dic_t, prob_DB_t, prob_bust_dic_t, prob_notbust_dic_t = h.init_probabilities(aiming_grid, prob_grid_normalscore_nt_pw, prob_grid_doublescore_nt_pw, prob_grid_bullscore_nt_pw, prob_grid_normalscore_t, prob_grid_doublescore_t, prob_grid_bullscore_t)\n",
    "\n",
    "param_pw = {}    \n",
    "\n",
    "prob_normalscore_tensor_nt_pw = torch.from_numpy(prob_normalscore_nt_pw)\n",
    "prob_normalscore_tensor_t = torch.from_numpy(prob_normalscore_t)\n",
    "param_pw['prob_normalscore_tensor_nt'] = prob_normalscore_tensor_nt_pw\n",
    "param_pw['prob_normalscore_tensor_t'] = prob_normalscore_tensor_t\n",
    "param_pw['prob_doublescore_dic_nt'] = prob_doublescore_dic_nt_pw\n",
    "param_pw['prob_doublescore_dic_t'] = prob_doublescore_dic_t\n",
    "param_pw['prob_DB_nt'] = prob_DB_nt_pw\n",
    "param_pw['prob_DB_t'] = prob_DB_t\n",
    "param_pw['prob_bust_dic_nt'] = prob_bust_dic_nt_pw\n",
    "param_pw['prob_bust_dic_t'] = prob_bust_dic_t\n",
    "    \n",
    "#### \n",
    "iteration_round_limit = 20\n",
    "iteration_relerror_limit = 10**-9\n",
    "\n",
    "\n",
    "value_pw = np.zeros((max_tokens+1,502,502))  # player A's winning probability when A throws at state [score_A, score_B]\n",
    "value_ps = np.zeros((max_tokens+1,502,502))  # player A's winning probability when B throws at state [score_A, score_B]\n",
    "value_win_pw = 1.0\n",
    "value_win_ps = 0.0\n",
    "game_begin_score_502 = 501+1\n",
    "num_iteration_record_pw = np.zeros((max_tokens+1,502,502), dtype=np.int8)\n",
    "\n",
    "state_len_vector_pw = np.zeros(4, dtype=np.int32)\n",
    "state_value_default  = [None]  ## expected # of turns for each state in the turn\n",
    "action_diff_pw  = [None]\n",
    "value_relerror_pw = np.zeros(4)\n",
    "\n",
    "for rt in [1,2,3]:\n",
    "    ## for rt=3: possible score_gained = 0\n",
    "    ## for rt=2: possible score_gained = 0,1,...,60\n",
    "    ## for rt=1: possible score_gained = 0,1,...,120\n",
    "    this_throw_state_len = fb.maxhitscore*(3-rt) + 1\n",
    "    state_value_default.append(np.ones((max_tokens+1,this_throw_state_len))*fb.largenumber)\n",
    "    action_diff_pw.append(np.ones((max_tokens+1,this_throw_state_len)))\n",
    "\n",
    "## for player A. (player B is fixed)\n",
    "## first key: score_state_pa=2,...,501; second key: score_state_pb=2,...,501; thrid key: throws=3,2,1\n",
    "optimal_value_dic = {} \n",
    "optimal_action_index_dic = {}\n",
    "prob_turn_transit_dic_pw = {}\n",
    "for score in range(2,502):\n",
    "    optimal_value_dic[score] = {}\n",
    "    optimal_action_index_dic[score] = {}\n",
    "    prob_turn_transit_dic_pw[score] = {}\n",
    "\n",
    " #### algorithm start ####\n",
    "t_policy_improvement = 0\n",
    "t_policy_evaluation = 0\n",
    "t_other = 0\n",
    "t1 = time.time()\n",
    "for score_state_ps in range(2, game_begin_score_502):\n",
    "    t_scoreloop_begin = time.time()\n",
    "    print('stronger state:',score_state_ps,'time:',t_scoreloop_begin-t1)\n",
    "    score_state_list = []\n",
    "\n",
    "    ## fix player B score, loop through player A\n",
    "    for score_state_pw in range(2, game_begin_score_502):\n",
    "\n",
    "        for tokens_pw in range(0,max_tokens+1):\n",
    "            \n",
    "            score_state_list.append([tokens_pw, score_state_pw, score_state_ps])\n",
    "\n",
    "    for [tokens_pw, score_state_pw, score_state_ps] in score_state_list:\n",
    "        # print('state',tokens_pw,score_state_pw,score_state_ps)\n",
    "\n",
    "        ## initialize player A initial policy:\n",
    "        for rt in [1,2,3]:        \n",
    "            this_throw_state_len_pa = min(score_state_pw-2, fb.maxhitscore*(3-rt)) + 1\n",
    "            state_len_vector_pw[rt] = this_throw_state_len_pa\n",
    "        state_value_pw = ft.copy_numberarray_container(state_value_default)\n",
    "        if score_state_ps > 2:\n",
    "            state_action_pw = ft.copy_numberarray_container(optimal_action_index_dic[score_state_pw][score_state_ps-1])\n",
    "            prob_turn_transit_pw = prob_turn_transit_dic_pw[score_state_pw][score_state_ps-1]\n",
    "        else:\n",
    "            state_action_pw = ft.copy_numberarray_container(dp_policy_dict_pw['optimal_action_index_dic'][score_state_pw])\n",
    "            prob_turn_transit_pw = dp_policy_dict_pw['prob_scorestate_transit'][tokens_pw][score_state_pw]\n",
    "        state_value_update_pw = ft.copy_numberarray_container(state_value_pw)\n",
    "        state_action_update_pw = ft.copy_numberarray_container(state_action_pw)\n",
    "\n",
    "        ## player B, turn score transit probability is fixed\n",
    "        prob_turn_transit_ps = dp_policy_dict_ps['prob_scorestate_transit'][0][score_state_ps] # assume no tokens \n",
    "\n",
    "        ## assemble variables\n",
    "        ## player A\n",
    "        param_pw['state_len_vector'] = state_len_vector_pw\n",
    "        param_pw['score_state'] = score_state_pw  \n",
    "        param_pw['token_state'] = tokens_pw   \n",
    "        param_pw['state_action'] = state_action_pw\n",
    "        param_pw['state_value'] = state_value_pw\n",
    "        param_pw['state_action_update'] = state_action_update_pw\n",
    "        param_pw['state_value_update'] = state_value_update_pw   \n",
    "        param_pw['action_diff'] = action_diff_pw\n",
    "        param_pw['value_relerror'] = value_relerror_pw        \n",
    "        ## maximize player A's win_prob\n",
    "        param_pw['flag_max'] = True\n",
    "        param_pw['next_turn_value'] = value_ps[:,score_state_ps] ## player B throws in next turn\n",
    "        param_pw['game_end_value'] = value_win_pw\n",
    "\n",
    "        ## policy iteration\n",
    "        for round_index in range(iteration_round_limit):            \n",
    "            \n",
    "            #### policy evaluation ####\n",
    "            tpe1 = time.time()\n",
    "            ## evaluate current policy, player A winning probability at (score_pa, score_pb, i=3, u=0)\n",
    "            ## value_pa: player A throws first, value_pb: player A throws second \n",
    "            ## player A, turn score transit probability                \n",
    "            ## use the initial prob_turn_transit_pa value for round_index=0\n",
    "            if (round_index >=0):\n",
    "                prob_turn_transit_pw = h.solve_turn_transit_probability_fast_token(score_state=score_state_pw,state_action=state_action_pw,available_tokens=tokens_pw,prob_grid_normalscore_nt=prob_grid_normalscore_nt_pw,prob_grid_doublescore_nt=prob_grid_doublescore_nt_pw,prob_grid_bullscore_nt=prob_grid_bullscore_nt_pw,prob_bust_dic_nt=prob_bust_dic_nt_pw,prob_grid_normalscore_t=prob_grid_normalscore_t,prob_grid_doublescore_t=prob_grid_doublescore_t,prob_grid_bullscore_t=prob_grid_bullscore_t,prob_bust_dic_t=prob_bust_dic_t)\n",
    "            [value_state_pw, value_state_ps] = zsg_policy_evaluation_tokens(value_pw, value_ps, tokens_pw, score_state_pw, score_state_ps, prob_turn_transit_pw, prob_turn_transit_ps)\n",
    "            value_pw[tokens_pw, score_state_pw, score_state_ps] = value_state_pw\n",
    "            value_ps[tokens_pw, score_state_pw, score_state_ps] = value_state_ps\n",
    "            tpe2 = time.time()\n",
    "            t_policy_evaluation += (tpe2-tpe1) \n",
    "\n",
    "            #### policy improvement for player A ####\n",
    "            tpi1 = time.time()\n",
    "            param_pw['round_index'] = round_index\n",
    "            [max_action_diff, max_value_relerror] = zsg_policy_improvement_tokens(param_pw)\n",
    "            tpi2 = time.time()\n",
    "            t_policy_improvement += (tpi2 - tpi1)                \n",
    "            if (max_action_diff < 1):\n",
    "                break\n",
    "            if (max_value_relerror < iteration_relerror_limit):\n",
    "                break\n",
    "\n",
    "        optimal_action_index_dic[score_state_pw][score_state_ps] = state_action_pw\n",
    "        optimal_value_dic[score_state_pw][score_state_ps] = state_value_pw\n",
    "        prob_turn_transit_dic_pw[score_state_pw][score_state_ps] = prob_turn_transit_pw\n",
    "        num_iteration_record_pw[tokens_pw, score_state_pw, score_state_ps] = round_index + 1\n",
    "\n",
    "## computation is done\n",
    "t2 = time.time()\n",
    "print('solve_zsg_opt_{}e{}_fix_{}e{} in {} seconds'.format(name_pw,epsilon_pw, name_ps, epsilon_ps, t2-t1))\n",
    "print('t_policy_evaluation  = {} seconds'.format(t_policy_evaluation))\n",
    "print('t_policy_improvement = {} seconds'.format(t_policy_improvement))\n",
    "print('t_other = {} seconds'.format(t_other))    \n",
    "#print('value_pa {} '.format(value_pa))\n",
    "#print('value_pb {} '.format(value_pb))\n",
    "\n",
    "result_dic = {'optimal_action_index_dic':optimal_action_index_dic, 'value_pw':value_pw, 'value_ps':value_ps,'optimal_value_dic':optimal_value_dic, 'info':info}    \n",
    "if (result_dir is not None):\n",
    "    ft.dump_pickle(result_filename, result_dic)\n",
    "    print('save {}'.format(result_filename))\n",
    "    ft.dump_pickle(result_value_filename, {'value_pw':value_pw, 'value_ps':value_ps, 'info':info})\n",
    "    print('save {}'.format(result_value_filename))\n",
    "    #return 'save'\n",
    "else:\n",
    "    #return result_dic\n",
    "    print('hello world')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_player10e2_S_player10e1_optS\n",
      "W_player10e1_S_player10e2_optW\n",
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_custom_no_tokens/player10_e1_gaussin_prob_grid_custom_no_tokens.pkl\n",
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_custom_no_tokens/player10_e2_gaussin_prob_grid_custom_no_tokens.pkl\n",
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_custom_tokens/t_gaussin_prob_grid_custom_tokens.pkl\n",
      "solve weaker player NS policy\n",
      "solve prob_policy_transit in 2.7756478786468506 seconds\n",
      "solve dp_turn_policyiter in 25.325114727020264 seconds\n",
      "[[0.     0.     1.8856 ... 5.5944 5.5758 5.6116]\n",
      " [0.     0.     1.     ... 4.9684 4.9631 4.9765]\n",
      " [0.     0.     1.     ... 4.7444 4.7403 4.7516]\n",
      " ...\n",
      " [0.     0.     1.     ... 3.8073 3.8027 3.8082]\n",
      " [0.     0.     1.     ... 3.5561 3.5447 3.5554]\n",
      " [0.     0.     1.     ... 3.     3.     3.    ]]\n",
      "solve stronger player NS policy\n",
      "solve prob_policy_transit in 3.7772469520568848 seconds\n",
      "solve dp_turn_policyiter in 27.889336109161377 seconds\n",
      "[[0.     0.     2.5371 ... 7.405  7.4122 7.4278]\n",
      " [0.     0.     1.     ... 6.2669 6.2766 6.2896]\n",
      " [0.     0.     1.     ... 5.8664 5.8755 5.8889]\n",
      " ...\n",
      " [0.     0.     1.     ... 3.9983 3.9972 4.0014]\n",
      " [0.     0.     1.     ... 3.7246 3.6864 3.6893]\n",
      " [0.     0.     1.     ... 3.     3.     3.    ]]\n",
      "stronger state: 2 time: 9.5367431640625e-07\n",
      "stronger state: 3 time: 2.1417298316955566\n",
      "stronger state: 4 time: 3.7240519523620605\n",
      "stronger state: 5 time: 5.275424003601074\n",
      "stronger state: 6 time: 6.88373589515686\n",
      "stronger state: 7 time: 8.61973786354065\n",
      "stronger state: 8 time: 10.386343717575073\n",
      "stronger state: 9 time: 12.109592914581299\n",
      "stronger state: 10 time: 14.090088844299316\n",
      "stronger state: 11 time: 15.96966290473938\n",
      "stronger state: 12 time: 17.781542778015137\n",
      "stronger state: 13 time: 19.32524275779724\n",
      "stronger state: 14 time: 21.19029688835144\n",
      "stronger state: 15 time: 23.0084388256073\n",
      "stronger state: 16 time: 24.743432760238647\n",
      "stronger state: 17 time: 26.508495807647705\n",
      "stronger state: 18 time: 28.192145824432373\n",
      "stronger state: 19 time: 29.82583498954773\n",
      "stronger state: 20 time: 31.568559885025024\n",
      "stronger state: 21 time: 33.24274778366089\n",
      "stronger state: 22 time: 34.95202684402466\n",
      "stronger state: 23 time: 36.76225686073303\n",
      "stronger state: 24 time: 38.49332785606384\n",
      "stronger state: 25 time: 40.254021883010864\n",
      "stronger state: 26 time: 41.947699785232544\n",
      "stronger state: 27 time: 43.59382081031799\n",
      "stronger state: 28 time: 45.35614895820618\n",
      "stronger state: 29 time: 47.026224851608276\n",
      "stronger state: 30 time: 48.604774713516235\n",
      "stronger state: 31 time: 50.37868690490723\n",
      "stronger state: 32 time: 52.14722490310669\n",
      "stronger state: 33 time: 53.792231798172\n",
      "stronger state: 34 time: 55.363603830337524\n",
      "stronger state: 35 time: 57.11918091773987\n",
      "stronger state: 36 time: 58.75355076789856\n",
      "stronger state: 37 time: 60.247159004211426\n",
      "stronger state: 38 time: 62.35244798660278\n",
      "stronger state: 39 time: 64.32315683364868\n",
      "stronger state: 40 time: 66.10083770751953\n",
      "stronger state: 41 time: 67.71316385269165\n",
      "stronger state: 42 time: 69.48774695396423\n",
      "stronger state: 43 time: 71.21243786811829\n",
      "stronger state: 44 time: 72.84177875518799\n",
      "stronger state: 45 time: 74.37931084632874\n",
      "stronger state: 46 time: 75.84075474739075\n",
      "stronger state: 47 time: 77.4451208114624\n",
      "stronger state: 48 time: 78.92676186561584\n",
      "stronger state: 49 time: 80.68575882911682\n",
      "stronger state: 50 time: 82.12537598609924\n",
      "stronger state: 51 time: 83.70453095436096\n",
      "stronger state: 52 time: 85.30256676673889\n",
      "stronger state: 53 time: 86.78726696968079\n",
      "stronger state: 54 time: 88.1887378692627\n",
      "stronger state: 55 time: 89.84359788894653\n",
      "stronger state: 56 time: 91.4867308139801\n",
      "stronger state: 57 time: 93.0880298614502\n",
      "stronger state: 58 time: 94.77063083648682\n",
      "stronger state: 59 time: 96.38666892051697\n",
      "stronger state: 60 time: 97.9090359210968\n",
      "stronger state: 61 time: 99.50379991531372\n",
      "stronger state: 62 time: 101.18096470832825\n",
      "stronger state: 63 time: 102.70188283920288\n",
      "stronger state: 64 time: 104.31222081184387\n",
      "stronger state: 65 time: 105.89137196540833\n",
      "stronger state: 66 time: 107.52667474746704\n",
      "stronger state: 67 time: 109.17061400413513\n",
      "stronger state: 68 time: 110.80406880378723\n",
      "stronger state: 69 time: 112.33257794380188\n",
      "stronger state: 70 time: 113.92382383346558\n",
      "stronger state: 71 time: 115.46759867668152\n",
      "stronger state: 72 time: 117.06335186958313\n",
      "stronger state: 73 time: 118.6245858669281\n",
      "stronger state: 74 time: 120.23774886131287\n",
      "stronger state: 75 time: 121.83507180213928\n",
      "stronger state: 76 time: 123.67959690093994\n",
      "stronger state: 77 time: 125.70842695236206\n",
      "stronger state: 78 time: 127.6037278175354\n",
      "stronger state: 79 time: 129.2100648880005\n",
      "stronger state: 80 time: 130.89583897590637\n",
      "stronger state: 81 time: 132.53206396102905\n",
      "stronger state: 82 time: 134.08675503730774\n",
      "stronger state: 83 time: 135.72107696533203\n",
      "stronger state: 84 time: 137.30286264419556\n",
      "stronger state: 85 time: 138.97975969314575\n",
      "stronger state: 86 time: 140.55826687812805\n",
      "stronger state: 87 time: 142.18041968345642\n",
      "stronger state: 88 time: 143.7802758216858\n",
      "stronger state: 89 time: 145.42606687545776\n",
      "stronger state: 90 time: 146.99110174179077\n",
      "stronger state: 91 time: 148.6228289604187\n",
      "stronger state: 92 time: 150.22362279891968\n",
      "stronger state: 93 time: 151.86521077156067\n",
      "stronger state: 94 time: 153.41873979568481\n",
      "stronger state: 95 time: 155.02934193611145\n",
      "stronger state: 96 time: 156.57551193237305\n",
      "stronger state: 97 time: 158.14372491836548\n",
      "stronger state: 98 time: 159.93760585784912\n",
      "stronger state: 99 time: 161.50738406181335\n",
      "stronger state: 100 time: 163.13242173194885\n",
      "stronger state: 101 time: 164.73990488052368\n",
      "stronger state: 102 time: 166.48813676834106\n",
      "stronger state: 103 time: 168.26827478408813\n",
      "stronger state: 104 time: 169.8546278476715\n",
      "stronger state: 105 time: 171.50998878479004\n",
      "stronger state: 106 time: 173.11197590827942\n",
      "stronger state: 107 time: 174.75994086265564\n",
      "stronger state: 108 time: 176.35918593406677\n",
      "stronger state: 109 time: 178.1466498374939\n",
      "stronger state: 110 time: 179.84063482284546\n",
      "stronger state: 111 time: 181.43368077278137\n",
      "stronger state: 112 time: 183.09298872947693\n",
      "stronger state: 113 time: 184.7754716873169\n",
      "stronger state: 114 time: 186.37202095985413\n",
      "stronger state: 115 time: 188.02217078208923\n",
      "stronger state: 116 time: 189.8634066581726\n",
      "stronger state: 117 time: 191.5955309867859\n",
      "stronger state: 118 time: 193.30014276504517\n",
      "stronger state: 119 time: 194.8959789276123\n",
      "stronger state: 120 time: 196.64314484596252\n",
      "stronger state: 121 time: 198.42982482910156\n",
      "stronger state: 122 time: 200.12703776359558\n",
      "stronger state: 123 time: 201.94179487228394\n",
      "stronger state: 124 time: 203.54554176330566\n",
      "stronger state: 125 time: 205.23885369300842\n",
      "stronger state: 126 time: 206.91098594665527\n",
      "stronger state: 127 time: 208.7285668849945\n",
      "stronger state: 128 time: 210.41026878356934\n",
      "stronger state: 129 time: 212.11105298995972\n",
      "stronger state: 130 time: 213.87929487228394\n",
      "stronger state: 131 time: 215.86110091209412\n",
      "stronger state: 132 time: 217.65030074119568\n",
      "stronger state: 133 time: 219.4474606513977\n",
      "stronger state: 134 time: 221.13325476646423\n",
      "stronger state: 135 time: 222.80474376678467\n",
      "stronger state: 136 time: 224.41289377212524\n",
      "stronger state: 137 time: 226.18712878227234\n",
      "stronger state: 138 time: 227.866849899292\n",
      "stronger state: 139 time: 229.5793309211731\n",
      "stronger state: 140 time: 231.22802305221558\n",
      "stronger state: 141 time: 232.86233067512512\n",
      "stronger state: 142 time: 234.52675771713257\n",
      "stronger state: 143 time: 236.10771870613098\n",
      "stronger state: 144 time: 237.74392080307007\n",
      "stronger state: 145 time: 239.35238981246948\n",
      "stronger state: 146 time: 241.02431273460388\n",
      "stronger state: 147 time: 242.58967089653015\n",
      "stronger state: 148 time: 244.2408058643341\n",
      "stronger state: 149 time: 245.81837797164917\n",
      "stronger state: 150 time: 247.45066285133362\n",
      "stronger state: 151 time: 249.08218598365784\n",
      "stronger state: 152 time: 250.73820686340332\n",
      "stronger state: 153 time: 252.3426947593689\n",
      "stronger state: 154 time: 254.00638103485107\n",
      "stronger state: 155 time: 255.62007069587708\n",
      "stronger state: 156 time: 257.2167248725891\n",
      "stronger state: 157 time: 258.82580184936523\n",
      "stronger state: 158 time: 260.52387404441833\n",
      "stronger state: 159 time: 262.0861587524414\n",
      "stronger state: 160 time: 263.7469928264618\n",
      "stronger state: 161 time: 265.3350348472595\n",
      "stronger state: 162 time: 267.27426981925964\n",
      "stronger state: 163 time: 269.02887868881226\n",
      "stronger state: 164 time: 270.64839696884155\n",
      "stronger state: 165 time: 272.30444169044495\n",
      "stronger state: 166 time: 273.9692769050598\n",
      "stronger state: 167 time: 275.62261605262756\n",
      "stronger state: 168 time: 277.44425892829895\n",
      "stronger state: 169 time: 279.02297377586365\n",
      "stronger state: 170 time: 280.59489583969116\n",
      "stronger state: 171 time: 282.3624997138977\n",
      "stronger state: 172 time: 283.9916820526123\n",
      "stronger state: 173 time: 285.63454365730286\n",
      "stronger state: 174 time: 287.343811750412\n",
      "stronger state: 175 time: 288.96846199035645\n",
      "stronger state: 176 time: 290.6697299480438\n",
      "stronger state: 177 time: 292.5798487663269\n",
      "stronger state: 178 time: 294.2727017402649\n",
      "stronger state: 179 time: 296.06630396842957\n",
      "stronger state: 180 time: 297.72499084472656\n",
      "stronger state: 181 time: 299.4898307323456\n",
      "stronger state: 182 time: 301.12090277671814\n",
      "stronger state: 183 time: 302.8186149597168\n",
      "stronger state: 184 time: 304.46667075157166\n",
      "stronger state: 185 time: 306.09818387031555\n",
      "stronger state: 186 time: 307.7551908493042\n",
      "stronger state: 187 time: 309.53344893455505\n",
      "stronger state: 188 time: 311.37801790237427\n",
      "stronger state: 189 time: 313.45619082450867\n",
      "stronger state: 190 time: 315.2016749382019\n",
      "stronger state: 191 time: 316.81405782699585\n",
      "stronger state: 192 time: 318.5547387599945\n",
      "stronger state: 193 time: 320.08418893814087\n",
      "stronger state: 194 time: 321.6847836971283\n",
      "stronger state: 195 time: 323.1983919143677\n",
      "stronger state: 196 time: 324.87820267677307\n",
      "stronger state: 197 time: 326.5534288883209\n",
      "stronger state: 198 time: 328.24710869789124\n",
      "stronger state: 199 time: 330.0223767757416\n",
      "stronger state: 200 time: 331.510498046875\n",
      "stronger state: 201 time: 333.3089437484741\n",
      "stronger state: 202 time: 335.1787598133087\n",
      "stronger state: 203 time: 336.86344480514526\n",
      "stronger state: 204 time: 338.60727882385254\n",
      "stronger state: 205 time: 340.4467658996582\n",
      "stronger state: 206 time: 342.46153593063354\n",
      "stronger state: 207 time: 344.38302993774414\n",
      "stronger state: 208 time: 346.33308005332947\n",
      "stronger state: 209 time: 348.3463559150696\n",
      "stronger state: 210 time: 350.1626408100128\n",
      "stronger state: 211 time: 351.90936398506165\n",
      "stronger state: 212 time: 353.7998037338257\n",
      "stronger state: 213 time: 355.70747685432434\n",
      "stronger state: 214 time: 357.5561988353729\n",
      "stronger state: 215 time: 359.28540897369385\n",
      "stronger state: 216 time: 361.05113887786865\n",
      "stronger state: 217 time: 362.9212510585785\n",
      "stronger state: 218 time: 364.7576439380646\n",
      "stronger state: 219 time: 366.687344789505\n",
      "stronger state: 220 time: 368.24771666526794\n",
      "stronger state: 221 time: 369.8010947704315\n",
      "stronger state: 222 time: 371.5147428512573\n",
      "stronger state: 223 time: 373.5253109931946\n",
      "stronger state: 224 time: 375.2578387260437\n",
      "stronger state: 225 time: 376.888751745224\n",
      "stronger state: 226 time: 378.766747713089\n",
      "stronger state: 227 time: 380.77818179130554\n",
      "stronger state: 228 time: 382.90336203575134\n",
      "stronger state: 229 time: 384.88200974464417\n",
      "stronger state: 230 time: 386.7501378059387\n",
      "stronger state: 231 time: 388.5450019836426\n",
      "stronger state: 232 time: 390.3780748844147\n",
      "stronger state: 233 time: 392.347562789917\n",
      "stronger state: 234 time: 394.06964778900146\n",
      "stronger state: 235 time: 395.88503193855286\n",
      "stronger state: 236 time: 398.1883430480957\n",
      "stronger state: 237 time: 400.2722580432892\n",
      "stronger state: 238 time: 402.2128508090973\n",
      "stronger state: 239 time: 404.3981719017029\n",
      "stronger state: 240 time: 406.21362471580505\n",
      "stronger state: 241 time: 408.02001190185547\n",
      "stronger state: 242 time: 409.7659709453583\n",
      "stronger state: 243 time: 411.7553107738495\n",
      "stronger state: 244 time: 413.9788830280304\n",
      "stronger state: 245 time: 415.9708888530731\n",
      "stronger state: 246 time: 417.838103055954\n",
      "stronger state: 247 time: 419.7815308570862\n",
      "stronger state: 248 time: 421.67517280578613\n",
      "stronger state: 249 time: 423.58814096450806\n",
      "stronger state: 250 time: 425.541198015213\n",
      "stronger state: 251 time: 427.25319600105286\n",
      "stronger state: 252 time: 429.0429120063782\n",
      "stronger state: 253 time: 430.7745409011841\n",
      "stronger state: 254 time: 432.5196228027344\n",
      "stronger state: 255 time: 434.1470437049866\n",
      "stronger state: 256 time: 435.8969259262085\n",
      "stronger state: 257 time: 437.7811608314514\n",
      "stronger state: 258 time: 439.5594139099121\n",
      "stronger state: 259 time: 441.48020601272583\n",
      "stronger state: 260 time: 443.44395780563354\n",
      "stronger state: 261 time: 445.615357875824\n",
      "stronger state: 262 time: 447.4125328063965\n",
      "stronger state: 263 time: 449.269868850708\n",
      "stronger state: 264 time: 451.16493678092957\n",
      "stronger state: 265 time: 453.0292956829071\n",
      "stronger state: 266 time: 454.8248598575592\n",
      "stronger state: 267 time: 456.5622138977051\n",
      "stronger state: 268 time: 458.03953409194946\n",
      "stronger state: 269 time: 459.55522298812866\n",
      "stronger state: 270 time: 461.103040933609\n",
      "stronger state: 271 time: 462.82122683525085\n",
      "stronger state: 272 time: 464.5921700000763\n",
      "stronger state: 273 time: 466.3133728504181\n",
      "stronger state: 274 time: 467.8863637447357\n",
      "stronger state: 275 time: 469.5408020019531\n",
      "stronger state: 276 time: 471.3737368583679\n",
      "stronger state: 277 time: 473.2067358493805\n",
      "stronger state: 278 time: 474.95692896842957\n",
      "stronger state: 279 time: 477.1975438594818\n",
      "stronger state: 280 time: 478.8855347633362\n",
      "stronger state: 281 time: 480.77814388275146\n",
      "stronger state: 282 time: 482.9581277370453\n",
      "stronger state: 283 time: 484.87936782836914\n",
      "stronger state: 284 time: 487.12902784347534\n",
      "stronger state: 285 time: 489.22232484817505\n",
      "stronger state: 286 time: 491.2872738838196\n",
      "stronger state: 287 time: 492.9509587287903\n",
      "stronger state: 288 time: 494.8269350528717\n",
      "stronger state: 289 time: 496.57840371131897\n",
      "stronger state: 290 time: 498.08942103385925\n",
      "stronger state: 291 time: 499.65454268455505\n",
      "stronger state: 292 time: 501.46969079971313\n",
      "stronger state: 293 time: 503.50728392601013\n",
      "stronger state: 294 time: 505.36084389686584\n",
      "stronger state: 295 time: 507.07683086395264\n",
      "stronger state: 296 time: 508.688973903656\n",
      "stronger state: 297 time: 510.65759682655334\n",
      "stronger state: 298 time: 512.3003838062286\n",
      "stronger state: 299 time: 514.402224779129\n",
      "stronger state: 300 time: 516.4606897830963\n",
      "stronger state: 301 time: 518.7395799160004\n",
      "stronger state: 302 time: 520.7279119491577\n",
      "stronger state: 303 time: 522.5199828147888\n",
      "stronger state: 304 time: 524.7877128124237\n",
      "stronger state: 305 time: 526.7906317710876\n",
      "stronger state: 306 time: 528.5917649269104\n",
      "stronger state: 307 time: 530.5372948646545\n",
      "stronger state: 308 time: 532.4055678844452\n",
      "stronger state: 309 time: 534.2030320167542\n",
      "stronger state: 310 time: 535.821124792099\n",
      "stronger state: 311 time: 537.5421359539032\n",
      "stronger state: 312 time: 539.304931640625\n",
      "stronger state: 313 time: 541.0033159255981\n",
      "stronger state: 314 time: 542.7483878135681\n",
      "stronger state: 315 time: 545.0111348628998\n",
      "stronger state: 316 time: 546.8749239444733\n",
      "stronger state: 317 time: 548.5750467777252\n",
      "stronger state: 318 time: 550.6009359359741\n",
      "stronger state: 319 time: 552.3321487903595\n",
      "stronger state: 320 time: 554.5330238342285\n",
      "stronger state: 321 time: 556.2305808067322\n",
      "stronger state: 322 time: 557.8300306797028\n",
      "stronger state: 323 time: 559.706524848938\n",
      "stronger state: 324 time: 561.1823678016663\n",
      "stronger state: 325 time: 562.7924237251282\n",
      "stronger state: 326 time: 565.0675597190857\n",
      "stronger state: 327 time: 566.7371919155121\n",
      "stronger state: 328 time: 568.1841316223145\n",
      "stronger state: 329 time: 569.9630227088928\n",
      "stronger state: 330 time: 571.5541250705719\n",
      "stronger state: 331 time: 573.223062992096\n",
      "stronger state: 332 time: 574.8316836357117\n",
      "stronger state: 333 time: 576.4928250312805\n",
      "stronger state: 334 time: 578.3229639530182\n",
      "stronger state: 335 time: 579.9927296638489\n",
      "stronger state: 336 time: 581.7398397922516\n",
      "stronger state: 337 time: 583.4612920284271\n",
      "stronger state: 338 time: 585.5487728118896\n",
      "stronger state: 339 time: 587.0829260349274\n",
      "stronger state: 340 time: 588.8023548126221\n",
      "stronger state: 341 time: 590.3534226417542\n",
      "stronger state: 342 time: 592.0729427337646\n",
      "stronger state: 343 time: 593.775242805481\n",
      "stronger state: 344 time: 595.5501868724823\n",
      "stronger state: 345 time: 597.1802537441254\n",
      "stronger state: 346 time: 598.8170869350433\n",
      "stronger state: 347 time: 600.4626438617706\n",
      "stronger state: 348 time: 602.056736946106\n",
      "stronger state: 349 time: 603.6357660293579\n",
      "stronger state: 350 time: 605.352166891098\n",
      "stronger state: 351 time: 606.8790237903595\n",
      "stronger state: 352 time: 608.5909459590912\n",
      "stronger state: 353 time: 610.2170298099518\n",
      "stronger state: 354 time: 611.9895088672638\n",
      "stronger state: 355 time: 613.4352240562439\n",
      "stronger state: 356 time: 615.0230598449707\n",
      "stronger state: 357 time: 616.7516117095947\n",
      "stronger state: 358 time: 618.5469627380371\n",
      "stronger state: 359 time: 620.1787118911743\n",
      "stronger state: 360 time: 621.9301128387451\n",
      "stronger state: 361 time: 623.7317748069763\n",
      "stronger state: 362 time: 625.7807779312134\n",
      "stronger state: 363 time: 627.844358921051\n",
      "stronger state: 364 time: 629.6898949146271\n",
      "stronger state: 365 time: 631.5247888565063\n",
      "stronger state: 366 time: 633.0259540081024\n",
      "stronger state: 367 time: 634.7030589580536\n",
      "stronger state: 368 time: 636.2368409633636\n",
      "stronger state: 369 time: 637.7604677677155\n",
      "stronger state: 370 time: 639.2928807735443\n",
      "stronger state: 371 time: 640.9368896484375\n",
      "stronger state: 372 time: 642.5664348602295\n",
      "stronger state: 373 time: 644.0018727779388\n",
      "stronger state: 374 time: 645.6934018135071\n",
      "stronger state: 375 time: 647.5273668766022\n",
      "stronger state: 376 time: 649.1883807182312\n",
      "stronger state: 377 time: 650.4802386760712\n",
      "stronger state: 378 time: 651.9050498008728\n",
      "stronger state: 379 time: 653.4898648262024\n",
      "stronger state: 380 time: 655.0937106609344\n",
      "stronger state: 381 time: 656.6545677185059\n",
      "stronger state: 382 time: 658.4000499248505\n",
      "stronger state: 383 time: 660.4879927635193\n",
      "stronger state: 384 time: 662.2861790657043\n",
      "stronger state: 385 time: 663.9356498718262\n",
      "stronger state: 386 time: 665.9956247806549\n",
      "stronger state: 387 time: 668.0932657718658\n",
      "stronger state: 388 time: 669.9326448440552\n",
      "stronger state: 389 time: 671.8047049045563\n",
      "stronger state: 390 time: 673.5276727676392\n",
      "stronger state: 391 time: 675.2803807258606\n",
      "stronger state: 392 time: 677.0855798721313\n",
      "stronger state: 393 time: 678.9604170322418\n",
      "stronger state: 394 time: 680.4686658382416\n",
      "stronger state: 395 time: 682.086345911026\n",
      "stronger state: 396 time: 683.755952835083\n",
      "stronger state: 397 time: 685.400768995285\n",
      "stronger state: 398 time: 686.9643549919128\n",
      "stronger state: 399 time: 688.6719279289246\n",
      "stronger state: 400 time: 690.2718739509583\n",
      "stronger state: 401 time: 692.1583368778229\n",
      "stronger state: 402 time: 693.9164688587189\n",
      "stronger state: 403 time: 695.879499912262\n",
      "stronger state: 404 time: 697.8638958930969\n",
      "stronger state: 405 time: 699.7935218811035\n",
      "stronger state: 406 time: 701.6296348571777\n",
      "stronger state: 407 time: 703.7180058956146\n",
      "stronger state: 408 time: 705.6338849067688\n",
      "stronger state: 409 time: 707.239403963089\n",
      "stronger state: 410 time: 708.932699918747\n",
      "stronger state: 411 time: 710.5749790668488\n",
      "stronger state: 412 time: 712.5422477722168\n",
      "stronger state: 413 time: 714.3925378322601\n",
      "stronger state: 414 time: 716.4535899162292\n",
      "stronger state: 415 time: 718.1793048381805\n",
      "stronger state: 416 time: 719.9331147670746\n",
      "stronger state: 417 time: 721.5303838253021\n",
      "stronger state: 418 time: 723.1771187782288\n",
      "stronger state: 419 time: 725.0524709224701\n",
      "stronger state: 420 time: 726.9452526569366\n",
      "stronger state: 421 time: 728.7820148468018\n",
      "stronger state: 422 time: 730.4838356971741\n",
      "stronger state: 423 time: 732.0570857524872\n",
      "stronger state: 424 time: 733.5769128799438\n",
      "stronger state: 425 time: 735.2207279205322\n",
      "stronger state: 426 time: 736.6949717998505\n",
      "stronger state: 427 time: 738.2753298282623\n",
      "stronger state: 428 time: 739.8209090232849\n",
      "stronger state: 429 time: 741.2248418331146\n",
      "stronger state: 430 time: 742.632031917572\n",
      "stronger state: 431 time: 743.964656829834\n",
      "stronger state: 432 time: 745.705778837204\n",
      "stronger state: 433 time: 747.2751259803772\n",
      "stronger state: 434 time: 748.9311900138855\n",
      "stronger state: 435 time: 750.2879989147186\n",
      "stronger state: 436 time: 752.0018637180328\n",
      "stronger state: 437 time: 753.535169839859\n",
      "stronger state: 438 time: 755.0237250328064\n",
      "stronger state: 439 time: 756.3563759326935\n",
      "stronger state: 440 time: 758.1935648918152\n",
      "stronger state: 441 time: 759.9504158496857\n",
      "stronger state: 442 time: 761.8176567554474\n",
      "stronger state: 443 time: 763.293744802475\n",
      "stronger state: 444 time: 764.769110918045\n",
      "stronger state: 445 time: 766.3661108016968\n",
      "stronger state: 446 time: 767.9507539272308\n",
      "stronger state: 447 time: 769.6473739147186\n",
      "stronger state: 448 time: 771.0406877994537\n",
      "stronger state: 449 time: 772.7279007434845\n",
      "stronger state: 450 time: 774.1011528968811\n",
      "stronger state: 451 time: 775.6449007987976\n",
      "stronger state: 452 time: 777.137482881546\n",
      "stronger state: 453 time: 778.6833899021149\n",
      "stronger state: 454 time: 780.3037958145142\n",
      "stronger state: 455 time: 781.7317957878113\n",
      "stronger state: 456 time: 783.0181498527527\n",
      "stronger state: 457 time: 784.5091738700867\n",
      "stronger state: 458 time: 785.767481803894\n",
      "stronger state: 459 time: 787.2684488296509\n",
      "stronger state: 460 time: 788.6425697803497\n",
      "stronger state: 461 time: 790.1412198543549\n",
      "stronger state: 462 time: 791.4850678443909\n",
      "stronger state: 463 time: 792.8308448791504\n",
      "stronger state: 464 time: 794.1730968952179\n",
      "stronger state: 465 time: 795.5928308963776\n",
      "stronger state: 466 time: 797.0158278942108\n",
      "stronger state: 467 time: 798.4024729728699\n",
      "stronger state: 468 time: 799.7940897941589\n",
      "stronger state: 469 time: 801.3634119033813\n",
      "stronger state: 470 time: 802.736555814743\n",
      "stronger state: 471 time: 804.428876876831\n",
      "stronger state: 472 time: 806.1913487911224\n",
      "stronger state: 473 time: 807.7599768638611\n",
      "stronger state: 474 time: 809.4454789161682\n",
      "stronger state: 475 time: 810.9042208194733\n",
      "stronger state: 476 time: 812.5901448726654\n",
      "stronger state: 477 time: 814.6146800518036\n",
      "stronger state: 478 time: 816.1277329921722\n",
      "stronger state: 479 time: 817.7695469856262\n",
      "stronger state: 480 time: 819.2440328598022\n",
      "stronger state: 481 time: 820.432131767273\n",
      "stronger state: 482 time: 821.8485078811646\n",
      "stronger state: 483 time: 823.1984677314758\n",
      "stronger state: 484 time: 824.7633919715881\n",
      "stronger state: 485 time: 826.7634928226471\n",
      "stronger state: 486 time: 828.3523128032684\n",
      "stronger state: 487 time: 829.7783327102661\n",
      "stronger state: 488 time: 831.3266568183899\n",
      "stronger state: 489 time: 832.6569509506226\n",
      "stronger state: 490 time: 834.0590398311615\n",
      "stronger state: 491 time: 835.4989619255066\n",
      "stronger state: 492 time: 837.3605906963348\n",
      "stronger state: 493 time: 838.8327188491821\n",
      "stronger state: 494 time: 840.4051198959351\n",
      "stronger state: 495 time: 841.8379986286163\n",
      "stronger state: 496 time: 843.2026498317719\n",
      "stronger state: 497 time: 844.4864008426666\n",
      "stronger state: 498 time: 845.8889148235321\n",
      "stronger state: 499 time: 847.4272758960724\n",
      "stronger state: 500 time: 849.0552990436554\n",
      "stronger state: 501 time: 850.3667449951172\n",
      "solve_zsg_opt_player10e1_fix_player10e2 in 851.5914659500122 seconds\n",
      "t_policy_evaluation  = 268.77818059921265 seconds\n",
      "t_policy_improvement = 575.4190902709961 seconds\n",
      "t_other = 0 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def solve_zsg_optW_fixNS(name_pw, name_ps, epsilon_pw, epsilon_ps, max_tokens_optimize=9, data_parameter_dir=fb.data_parameter_dir, dp_policy_folder=None, result_dir=None, postfix='', gpu_device=None):\n",
    "    \n",
    "    max_tokens=max_tokens_optimize\n",
    "    game_begin_score_502 = 501+1\n",
    "    \n",
    "    info = 'W_{}e{}_S_{}e{}_optW'.format(name_pw, epsilon_pw, name_ps, epsilon_ps)\n",
    "    print(info)\n",
    "    ##\n",
    "    if result_dir is not None:\n",
    "        if not os.path.isdir(result_dir):\n",
    "            os.makedirs(result_dir)\n",
    "        result_filename = result_dir + '/zsg_W_{}e{}_S_{}e{}_{}_optW.pkl'.format(name_pw, epsilon_pw, name_ps, epsilon_ps, postfix)\n",
    "        result_value_filename = result_dir + '/zsg_value_W_{}e{}_S_{}e{}_{}_optW.pkl'.format(name_pw, epsilon_pw, name_ps, epsilon_ps, postfix)\n",
    "\n",
    "\n",
    "    [aiming_grid, prob_grid_normalscore_nt_pw, prob_grid_singlescore_nt_pw, prob_grid_doublescore_nt_pw, prob_grid_triplescore_nt_pw, prob_grid_bullscore_nt_pw] = h.load_aiming_grid(name_pw, epsilon=epsilon_pw, data_parameter_dir=data_parameter_dir, grid_version='custom_no_tokens')\n",
    "    [aiming_grid, prob_grid_normalscore_nt_ps, prob_grid_singlescore_nt_ps, prob_grid_doublescore_nt_ps, prob_grid_triplescore_nt_ps, prob_grid_bullscore_nt_ps] = h.load_aiming_grid(name_ps, epsilon=epsilon_ps, data_parameter_dir=data_parameter_dir, grid_version='custom_no_tokens')\n",
    "    [aiming_grid, prob_grid_normalscore_t, prob_grid_singlescore_t, prob_grid_doublescore_t, prob_grid_triplescore_t, prob_grid_bullscore_t] = h.load_aiming_grid('t', data_parameter_dir=data_parameter_dir, grid_version='custom_tokens')\n",
    "\n",
    "    ## use single player game as the fixed policy    \n",
    "    dp_policy_dict_pw, dp_policy_dict_ps = load_ns_policy_dicts(name_pw,name_ps,epsilon_pw,epsilon_ps,dp_policy_folder,aiming_grid, prob_grid_normalscore_nt_pw, prob_grid_singlescore_nt_pw, prob_grid_doublescore_nt_pw, prob_grid_triplescore_nt_pw, prob_grid_bullscore_nt_pw, prob_grid_normalscore_nt_ps, prob_grid_singlescore_nt_ps, prob_grid_doublescore_nt_ps, prob_grid_triplescore_nt_ps, prob_grid_bullscore_nt_ps, prob_grid_normalscore_t, prob_grid_singlescore_t, prob_grid_doublescore_t, prob_grid_triplescore_t, prob_grid_bullscore_t)\n",
    "\n",
    "\n",
    "    num_aiming_location_pw, prob_normalscore_nt_pw, prob_doublescore_dic_nt_pw, prob_DB_nt_pw, prob_bust_dic_nt_pw, prob_notbust_dic_nt_pw, prob_normalscore_t, prob_doublescore_dic_t, prob_DB_t, prob_bust_dic_t, prob_notbust_dic_t = h.init_probabilities(aiming_grid, prob_grid_normalscore_nt_pw, prob_grid_doublescore_nt_pw, prob_grid_bullscore_nt_pw, prob_grid_normalscore_t, prob_grid_doublescore_t, prob_grid_bullscore_t)\n",
    "\n",
    "    param_pw = {}    \n",
    "\n",
    "    prob_normalscore_tensor_nt_pw = torch.from_numpy(prob_normalscore_nt_pw)\n",
    "    prob_normalscore_tensor_t = torch.from_numpy(prob_normalscore_t)\n",
    "    param_pw['prob_normalscore_tensor_nt'] = prob_normalscore_tensor_nt_pw\n",
    "    param_pw['prob_normalscore_tensor_t'] = prob_normalscore_tensor_t\n",
    "    param_pw['prob_doublescore_dic_nt'] = prob_doublescore_dic_nt_pw\n",
    "    param_pw['prob_doublescore_dic_t'] = prob_doublescore_dic_t\n",
    "    param_pw['prob_DB_nt'] = prob_DB_nt_pw\n",
    "    param_pw['prob_DB_t'] = prob_DB_t\n",
    "    param_pw['prob_bust_dic_nt'] = prob_bust_dic_nt_pw\n",
    "    param_pw['prob_bust_dic_t'] = prob_bust_dic_t\n",
    "        \n",
    "    #### \n",
    "    iteration_round_limit = 20\n",
    "    iteration_relerror_limit = 10**-9\n",
    "\n",
    "    value_pw = np.zeros((max_tokens+1,game_begin_score_502,game_begin_score_502))  # player A's winning probability when A throws at state [score_A, score_B]\n",
    "    value_ps = np.zeros((max_tokens+1,game_begin_score_502,game_begin_score_502))  # player A's winning probability when B throws at state [score_A, score_B]\n",
    "    value_win_pw = 1.0\n",
    "    value_win_ps = 0.0\n",
    "    num_iteration_record_pw = np.zeros((max_tokens+1,game_begin_score_502,game_begin_score_502), dtype=np.int8)\n",
    "\n",
    "    state_len_vector_pw = np.zeros(4, dtype=np.int32)\n",
    "    state_value_default  = [None]  ## expected # of turns for each state in the turn\n",
    "    action_diff_pw  = [None]\n",
    "    value_relerror_pw = np.zeros(4)\n",
    "\n",
    "    for rt in [1,2,3]:\n",
    "        ## for rt=3: possible score_gained = 0\n",
    "        ## for rt=2: possible score_gained = 0,1,...,60\n",
    "        ## for rt=1: possible score_gained = 0,1,...,120\n",
    "        this_throw_state_len = fb.maxhitscore*(3-rt) + 1\n",
    "        state_value_default.append(np.ones((max_tokens+1,this_throw_state_len))*fb.largenumber)\n",
    "        action_diff_pw.append(np.ones((max_tokens+1,this_throw_state_len)))\n",
    "\n",
    "    ## for player A. (player B is fixed)\n",
    "    ## first key: score_state_pa=2,...,501; second key: score_state_pb=2,...,501; thrid key: throws=3,2,1\n",
    "    optimal_value_dic = {} \n",
    "    optimal_action_index_dic = {}\n",
    "    prob_turn_transit_dic_pw = {}\n",
    "    for score in range(2,502):\n",
    "        optimal_value_dic[score] = {}\n",
    "        optimal_action_index_dic[score] = {}\n",
    "        prob_turn_transit_dic_pw[score] = {}\n",
    "\n",
    "    #### algorithm start ####\n",
    "    t_policy_improvement = 0\n",
    "    t_policy_evaluation = 0\n",
    "    t_other = 0\n",
    "    t1 = time.time()\n",
    "    for score_state_ps in range(2, game_begin_score_502):\n",
    "        t_scoreloop_begin = time.time()\n",
    "        print('stronger state:',score_state_ps,'time:',t_scoreloop_begin-t1)\n",
    "        score_state_list = []\n",
    "\n",
    "        ## fix player B score, loop through player A\n",
    "        for score_state_pw in range(2, game_begin_score_502):\n",
    "\n",
    "            for tokens_pw in range(0,max_tokens+1):\n",
    "                \n",
    "                score_state_list.append([tokens_pw, score_state_pw, score_state_ps])\n",
    "\n",
    "        for [tokens_pw, score_state_pw, score_state_ps] in score_state_list:\n",
    "            # print('state',tokens_pw,score_state_pw,score_state_ps)\n",
    "\n",
    "            ## initialize player A initial policy:\n",
    "            for rt in [1,2,3]:        \n",
    "                this_throw_state_len_pw = min(score_state_pw-2, fb.maxhitscore*(3-rt)) + 1\n",
    "                state_len_vector_pw[rt] = this_throw_state_len_pw\n",
    "            state_value_pw = ft.copy_numberarray_container(state_value_default)\n",
    "            if score_state_ps > 2:\n",
    "                state_action_pw = ft.copy_numberarray_container(optimal_action_index_dic[score_state_pw][score_state_ps-1])\n",
    "                prob_turn_transit_pw = prob_turn_transit_dic_pw[score_state_pw][score_state_ps-1]\n",
    "            else:\n",
    "                state_action_pw = ft.copy_numberarray_container(dp_policy_dict_pw['optimal_action_index_dic'][score_state_pw])\n",
    "                prob_turn_transit_pw = dp_policy_dict_pw['prob_scorestate_transit'][tokens_pw][score_state_pw]\n",
    "            state_value_update_pw = ft.copy_numberarray_container(state_value_pw)\n",
    "            state_action_update_pw = ft.copy_numberarray_container(state_action_pw)\n",
    "\n",
    "            ## player B, turn score transit probability is fixed\n",
    "            prob_turn_transit_ps = dp_policy_dict_ps['prob_scorestate_transit'][0][score_state_ps] # assume no tokens \n",
    "\n",
    "            ## assemble variables\n",
    "            ## player A\n",
    "            param_pw['state_len_vector'] = state_len_vector_pw\n",
    "            param_pw['score_state'] = score_state_pw  \n",
    "            param_pw['token_state'] = tokens_pw   \n",
    "            param_pw['state_action'] = state_action_pw\n",
    "            param_pw['state_value'] = state_value_pw\n",
    "            param_pw['state_action_update'] = state_action_update_pw\n",
    "            param_pw['state_value_update'] = state_value_update_pw   \n",
    "            param_pw['action_diff'] = action_diff_pw\n",
    "            param_pw['value_relerror'] = value_relerror_pw        \n",
    "            ## maximize player A's win_prob\n",
    "            param_pw['flag_max'] = True\n",
    "            param_pw['next_turn_value'] = value_ps[:,score_state_ps] ## player B throws in next turn\n",
    "            param_pw['game_end_value'] = value_win_pw\n",
    "\n",
    "            ## policy iteration\n",
    "            for round_index in range(iteration_round_limit):            \n",
    "                \n",
    "                #### policy evaluation ####\n",
    "                tpe1 = time.time()\n",
    "                ## evaluate current policy, player A winning probability at (score_pa, score_pb, i=3, u=0)\n",
    "                ## value_pa: player A throws first, value_pb: player A throws second \n",
    "                ## player A, turn score transit probability                \n",
    "                ## use the initial prob_turn_transit_pa value for round_index=0\n",
    "                if (round_index >=0):\n",
    "                    prob_turn_transit_pw = h.solve_turn_transit_probability_fast_token(score_state=score_state_pw,state_action=state_action_pw,available_tokens=tokens_pw,prob_grid_normalscore_nt=prob_grid_normalscore_nt_pw,prob_grid_doublescore_nt=prob_grid_doublescore_nt_pw,prob_grid_bullscore_nt=prob_grid_bullscore_nt_pw,prob_bust_dic_nt=prob_bust_dic_nt_pw,prob_grid_normalscore_t=prob_grid_normalscore_t,prob_grid_doublescore_t=prob_grid_doublescore_t,prob_grid_bullscore_t=prob_grid_bullscore_t,prob_bust_dic_t=prob_bust_dic_t)\n",
    "                [value_state_pw, value_state_ps] = zsg_policy_evaluation_tokens(value_pw, value_ps, tokens_pw, score_state_pw, score_state_ps, prob_turn_transit_pw, prob_turn_transit_ps)\n",
    "                value_pw[tokens_pw, score_state_pw, score_state_ps] = value_state_pw\n",
    "                value_ps[tokens_pw, score_state_pw, score_state_ps] = value_state_ps\n",
    "                tpe2 = time.time()\n",
    "                t_policy_evaluation += (tpe2-tpe1) \n",
    "\n",
    "                #### policy improvement for player A ####\n",
    "                tpi1 = time.time()\n",
    "                param_pw['round_index'] = round_index\n",
    "                [max_action_diff, max_value_relerror] = zsg_policy_improvement_tokens(param_pw)\n",
    "                tpi2 = time.time()\n",
    "                t_policy_improvement += (tpi2 - tpi1)                \n",
    "                if (max_action_diff < 1):\n",
    "                    break\n",
    "                if (max_value_relerror < iteration_relerror_limit):\n",
    "                    break\n",
    "\n",
    "            optimal_action_index_dic[score_state_pw][score_state_ps] = state_action_pw\n",
    "            optimal_value_dic[score_state_pw][score_state_ps] = state_value_pw\n",
    "            prob_turn_transit_dic_pw[score_state_pw][score_state_ps] = prob_turn_transit_pw\n",
    "            num_iteration_record_pw[tokens_pw, score_state_pw, score_state_ps] = round_index + 1\n",
    "\n",
    "    ## computation is done\n",
    "    t2 = time.time()\n",
    "    print('solve_zsg_opt_{}e{}_fix_{}e{} in {} seconds'.format(name_pw,epsilon_pw, name_ps, epsilon_ps, t2-t1))\n",
    "    print('t_policy_evaluation  = {} seconds'.format(t_policy_evaluation))\n",
    "    print('t_policy_improvement = {} seconds'.format(t_policy_improvement))\n",
    "    print('t_other = {} seconds'.format(t_other))    \n",
    "    #print('value_pa {} '.format(value_pa))\n",
    "    #print('value_pb {} '.format(value_pb))\n",
    "\n",
    "    result_dic = {'optimal_action_index_dic':optimal_action_index_dic, 'value_pw':value_pw, 'value_ps':value_ps,'optimal_value_dic':optimal_value_dic, 'info':info}    \n",
    "    if (result_dir is not None):\n",
    "        ft.dump_pickle(result_filename, result_dic)\n",
    "        print('save {}'.format(result_filename))\n",
    "        ft.dump_pickle(result_value_filename, {'value_pw':value_pw, 'value_ps':value_ps, 'info':info})\n",
    "        print('save {}'.format(result_value_filename))\n",
    "        return 'save'\n",
    "    else:\n",
    "        return result_dic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "solve_zsg_optW_fixNS(name_pw, name_ps, epsilon_pw, epsilon_ps, max_tokens_optimize=9, data_parameter_dir=fb.data_parameter_dir, dp_policy_folder='result', result_dir='result_zsg', postfix='', gpu_device=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.0 Solve BR for PS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_player10e2_S_player10e1_optboth\n",
      "player W is player10 e2 and player S is player10 e1\n",
      "optimize both players\n",
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_custom_no_tokens/player10_e2_gaussin_prob_grid_custom_no_tokens.pkl\n",
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_custom_no_tokens/player10_e1_gaussin_prob_grid_custom_no_tokens.pkl\n",
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_custom_tokens/t_gaussin_prob_grid_custom_tokens.pkl\n",
      "load weaker player policy result/singlegame_player10_e2_turn_tokens.pkl\n",
      "load stronger player policy result/singlegame_player10_e1_turn_tokens.pkl\n",
      "stronger state: 2 time: 4.0531158447265625e-06\n",
      "stronger state: 3 time: 39.03641700744629\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[163], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m solve_zsg_optboth(name_pw, name_ps, epsilon_pw, epsilon_ps, data_parameter_dir\u001B[39m=\u001B[39;49mfb\u001B[39m.\u001B[39;49mdata_parameter_dir, dp_policy_folder\u001B[39m=\u001B[39;49m\u001B[39m'\u001B[39;49m\u001B[39mresult\u001B[39;49m\u001B[39m'\u001B[39;49m, result_dir\u001B[39m=\u001B[39;49m\u001B[39m'\u001B[39;49m\u001B[39mresult\u001B[39;49m\u001B[39m'\u001B[39;49m, postfix\u001B[39m=\u001B[39;49m\u001B[39m'\u001B[39;49m\u001B[39m'\u001B[39;49m, gpu_device\u001B[39m=\u001B[39;49m\u001B[39mNone\u001B[39;49;00m)\n",
      "Cell \u001B[0;32mIn[162], line 207\u001B[0m, in \u001B[0;36msolve_zsg_optboth\u001B[0;34m(name_pw, name_ps, epsilon_pw, epsilon_ps, data_parameter_dir, dp_policy_folder, result_dir, postfix, gpu_device)\u001B[0m\n\u001B[1;32m    205\u001B[0m \u001B[39m## use the initial prob_turn_transit_pa value for round_index=0\u001B[39;00m\n\u001B[1;32m    206\u001B[0m \u001B[39mif\u001B[39;00m (round_index \u001B[39m>\u001B[39m\u001B[39m=\u001B[39m\u001B[39m0\u001B[39m):\n\u001B[0;32m--> 207\u001B[0m     prob_turn_transit_pw \u001B[39m=\u001B[39m h\u001B[39m.\u001B[39;49msolve_turn_transit_probability_fast_token(score_state\u001B[39m=\u001B[39;49mscore_state_pw,state_action\u001B[39m=\u001B[39;49mstate_action_pw,available_tokens\u001B[39m=\u001B[39;49mtokens_pw,prob_grid_normalscore_nt\u001B[39m=\u001B[39;49mprob_grid_normalscore_nt_pw,prob_grid_doublescore_nt\u001B[39m=\u001B[39;49mprob_grid_doublescore_nt_pw,prob_grid_bullscore_nt\u001B[39m=\u001B[39;49mprob_grid_bullscore_nt_pw,prob_bust_dic_nt\u001B[39m=\u001B[39;49mprob_bust_dic_nt_pw,prob_grid_normalscore_t\u001B[39m=\u001B[39;49mprob_grid_normalscore_t,prob_grid_doublescore_t\u001B[39m=\u001B[39;49mprob_grid_doublescore_t,prob_grid_bullscore_t\u001B[39m=\u001B[39;49mprob_grid_bullscore_t,prob_bust_dic_t\u001B[39m=\u001B[39;49mprob_bust_dic_t)\n\u001B[1;32m    208\u001B[0m \u001B[39m## player B is fixed, use stored value\u001B[39;00m\n\u001B[1;32m    209\u001B[0m [value_state_pw, value_state_ps] \u001B[39m=\u001B[39m zsg_policy_evaluation_tokens(value_pw, value_ps, tokens_pw, score_state_pw, score_state_ps, prob_turn_transit_pw, prob_turn_transit_ps)\n",
      "File \u001B[0;32m~/Desktop/darts-thesis/helpers.py:539\u001B[0m, in \u001B[0;36msolve_turn_transit_probability_fast_token\u001B[0;34m(score_state, state_action, available_tokens, prob_grid_normalscore_nt, prob_grid_doublescore_nt, prob_grid_bullscore_nt, prob_bust_dic_nt, prob_grid_normalscore_t, prob_grid_doublescore_t, prob_grid_bullscore_t, prob_bust_dic_t)\u001B[0m\n\u001B[1;32m    534\u001B[0m \u001B[39mfor\u001B[39;00m tokens_used \u001B[39min\u001B[39;00m \u001B[39mrange\u001B[39m(parent_probability\u001B[39m.\u001B[39mshape[\u001B[39m0\u001B[39m]):\n\u001B[1;32m    536\u001B[0m     \u001B[39mfor\u001B[39;00m score_gained \u001B[39min\u001B[39;00m \u001B[39mrange\u001B[39m(parent_probability\u001B[39m.\u001B[39mshape[\u001B[39m1\u001B[39m]):\n\u001B[1;32m    537\u001B[0m \n\u001B[1;32m    538\u001B[0m         \u001B[39m## skip infeasible state\u001B[39;00m\n\u001B[0;32m--> 539\u001B[0m         \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m fb\u001B[39m.\u001B[39mstate_feasible_array[rt, score_gained]:\n\u001B[1;32m    540\u001B[0m             \u001B[39mcontinue\u001B[39;00m   \n\u001B[1;32m    542\u001B[0m         \u001B[39m## skip if zero probability of being in parent state in the first place to save time\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_player10e2_S_player10e1_optboth\n",
      "player W is player10 e2 and player S is player10 e1\n",
      "optimize both players\n",
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_custom_no_tokens/player10_e2_gaussin_prob_grid_custom_no_tokens.pkl\n",
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_custom_no_tokens/player10_e1_gaussin_prob_grid_custom_no_tokens.pkl\n",
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_custom_tokens/t_gaussin_prob_grid_custom_tokens.pkl\n",
      "load weaker player policy result/singlegame_player10_e2_turn_tokens.pkl\n",
      "load stronger player policy result/singlegame_player10_e1_turn_tokens.pkl\n",
      "stronger state: 2 time: 0.0006747245788574219\n",
      "stronger state: 3 time: 40.13592171669006\n",
      "stronger state: 4 time: 85.01364469528198\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[161], line 215\u001B[0m\n\u001B[1;32m    213\u001B[0m tpi1 \u001B[39m=\u001B[39m time\u001B[39m.\u001B[39mtime()\n\u001B[1;32m    214\u001B[0m param_pw[\u001B[39m'\u001B[39m\u001B[39mround_index\u001B[39m\u001B[39m'\u001B[39m] \u001B[39m=\u001B[39m round_index\n\u001B[0;32m--> 215\u001B[0m [max_action_diff_pw, max_value_relerror_pw] \u001B[39m=\u001B[39m zsg_policy_improvement_tokens(param_pw)\n\u001B[1;32m    216\u001B[0m tpi2 \u001B[39m=\u001B[39m time\u001B[39m.\u001B[39mtime()\n\u001B[1;32m    217\u001B[0m t_policy_improvement \u001B[39m+\u001B[39m\u001B[39m=\u001B[39m (tpi2 \u001B[39m-\u001B[39m tpi1)\n",
      "Cell \u001B[0;32mIn[50], line 247\u001B[0m, in \u001B[0;36mzsg_policy_improvement_tokens\u001B[0;34m(param)\u001B[0m\n\u001B[1;32m    245\u001B[0m         temp1 \u001B[39m=\u001B[39m win_prob_tensor[:imdp\u001B[39m.\u001B[39mthrow_num]\u001B[39m.\u001B[39mmax(axis\u001B[39m=\u001B[39m\u001B[39m0\u001B[39m)\n\u001B[1;32m    246\u001B[0m     \u001B[39melse\u001B[39;00m: \n\u001B[0;32m--> 247\u001B[0m         temp1 \u001B[39m=\u001B[39m win_prob_tensor\u001B[39m.\u001B[39;49mmax(axis\u001B[39m=\u001B[39;49m\u001B[39m0\u001B[39;49m)\n\u001B[1;32m    248\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m    249\u001B[0m     \u001B[39mif\u001B[39;00m token_state \u001B[39m==\u001B[39m \u001B[39m0\u001B[39m: \n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "## fix player A's Naive Strategy (NS) and optimize player B\n",
    "def solve_zsg_optS_fixNS(name_pw, name_ps, epsilon_pw, epsilon_ps, data_parameter_dir=fb.data_parameter_dir, dp_policy_folder=None, result_dir=None, postfix='', gpu_device=None):\n",
    "    \n",
    "    max_tokens_stronger=0\n",
    "    \n",
    "    info = 'W_{}e{}_S_{}e{}_optS'.format(name_pw, epsilon_pw, name_ps, epsilon_ps)\n",
    "    print(info)\n",
    "    ##\n",
    "    if result_dir is not None:    \n",
    "        if not os.path.isdir(result_dir):\n",
    "            os.makedirs(result_dir)\n",
    "        result_filename = result_dir + '/zsg_W_{}e{}_S_{}e{}_{}_optS.pkl'.format(name_pw, epsilon_pw, name_ps, epsilon_ps, postfix)\n",
    "        result_value_filename = result_dir + '/zsg_value_W_{}e{}_S_{}e{}_{}_optS.pkl'.format(name_pw, epsilon_pw, name_ps, epsilon_ps, postfix)\n",
    "\n",
    "    ## need to reset the result key name: Player A is name_pb and Player B is name_pa\n",
    "    ## game values are represented in terms of Player A's winning probability\n",
    "    temp_result_dic = solve_zsg_optW_fixNS(name_ps, name_pw, epsilon_ps, epsilon_pw, max_tokens_optimize=max_tokens_stronger, dp_policy_folder=dp_policy_folder, result_dir=None, postfix='', gpu_device=gpu_device)\n",
    "    result_dic = {'info':info}\n",
    "    value_pw = 1-temp_result_dic['value_ps'].T    \n",
    "    value_ps = 1-temp_result_dic['value_pw'].T\n",
    "    value_pw[:2,:] = 0\n",
    "    value_pw[:,:2] = 0\n",
    "    value_pw[:2,:] = 0\n",
    "    value_pw[:,:2] = 0\n",
    "    \n",
    "    result_dic['value_pw'] = value_pw\n",
    "    result_dic['value_ps'] = value_ps   \n",
    "    result_dic['optimal_action_index_dic'] = {}\n",
    "    result_dic['optimal_value_dic'] = {}    \n",
    "    for score_state_pw in range(2, game_begin_score_502):\n",
    "    #for score_state_pa in range(2, 101):\n",
    "        result_dic['optimal_action_index_dic'][score_state_pw] = {}\n",
    "        result_dic['optimal_value_dic'][score_state_pw] = {}\n",
    "        for score_state_ps in range(2, game_begin_score_502):\n",
    "        #for score_state_pb in range(2, 101):\n",
    "            result_dic['optimal_action_index_dic'][score_state_pw][score_state_ps] = temp_result_dic['optimal_action_index_dic'][score_state_ps][score_state_pw]\n",
    "            result_dic['optimal_value_dic'][score_state_pw][score_state_ps] = temp_result_dic['optimal_value_dic'][score_state_ps][score_state_pw]\n",
    "    \n",
    "    if (result_dir is not None):\n",
    "        ft.dump_pickle(result_filename, result_dic)\n",
    "        print('save {}'.format(result_filename))\n",
    "        ft.dump_pickle(result_value_filename, {'value_pa':value_pw, 'value_pb':value_ps, 'info':info})\n",
    "        print('save {}'.format(result_value_filename))\n",
    "        return 'save'\n",
    "    else:\n",
    "        return result_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([2])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dic = solve_zsg_optS_fixNS(name_pw, name_ps, epsilon_pw, epsilon_ps, data_parameter_dir=fb.data_parameter_dir, dp_policy_folder=None, result_dir=None, postfix='', gpu_device=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.0 Full ZSG"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "## optimize player A and B alternatively until achieving optimal\n",
    "def solve_zsg_optboth(name_pw, name_ps, epsilon_pw, epsilon_ps, data_parameter_dir=fb.data_parameter_dir, dp_policy_folder=None, result_dir=None, postfix='', gpu_device=None):\n",
    "    info = 'W_{}e{}_S_{}e{}_optboth'.format(name_pw, epsilon_pw, name_ps, epsilon_ps)\n",
    "    print(info)\n",
    "    ##\n",
    "    if result_dir is not None:\n",
    "        if not os.path.isdir(result_dir):\n",
    "            os.makedirs(result_dir)\n",
    "        result_filename = result_dir + '/zsg_W_{}e{}_S_{}e{}_{}_optboth.pkl'.format(name_pw, epsilon_pw, name_ps, epsilon_ps, postfix)\n",
    "        result_value_filename = result_dir + '/zsg_value_W_{}e{}_S_{}e{}_{}_optboth.pkl'.format(name_pw, epsilon_pw, name_ps, epsilon_ps, postfix)\n",
    "\n",
    "\n",
    "    max_tokens = 9\n",
    "    game_begin_score_502 = 501+1\n",
    "    #player A: pa throw first\n",
    "    #player B: pb throw after player A, policy is fixed as ns\n",
    "    print('player W is {} e{} and player S is {} e{}'.format(name_pw, epsilon_pw, name_ps, epsilon_ps))\n",
    "    print('optimize both players')\n",
    "\n",
    "    [aiming_grid, prob_grid_normalscore_nt_pw, prob_grid_singlescore_nt_pw, prob_grid_doublescore_nt_pw, prob_grid_triplescore_nt_pw, prob_grid_bullscore_nt_pw] = h.load_aiming_grid(name_pw, epsilon=epsilon_pw, data_parameter_dir=data_parameter_dir, grid_version='custom_no_tokens')\n",
    "    [aiming_grid, prob_grid_normalscore_nt_ps, prob_grid_singlescore_nt_ps, prob_grid_doublescore_nt_ps, prob_grid_triplescore_nt_ps, prob_grid_bullscore_nt_ps] = h.load_aiming_grid(name_ps, epsilon=epsilon_ps, data_parameter_dir=data_parameter_dir, grid_version='custom_no_tokens')\n",
    "    [aiming_grid, prob_grid_normalscore_t, prob_grid_singlescore_t, prob_grid_doublescore_t, prob_grid_triplescore_t, prob_grid_bullscore_t] = h.load_aiming_grid('t', data_parameter_dir=data_parameter_dir, grid_version='custom_tokens')\n",
    "\n",
    "    ## use single player game as the fixed policy    \n",
    "    dp_policy_dict_pw, dp_policy_dict_ps = load_ns_policy_dicts(name_pw,name_ps,epsilon_pw,epsilon_ps,dp_policy_folder,aiming_grid, prob_grid_normalscore_nt_pw, prob_grid_singlescore_nt_pw, prob_grid_doublescore_nt_pw, prob_grid_triplescore_nt_pw, prob_grid_bullscore_nt_pw, prob_grid_normalscore_nt_ps, prob_grid_singlescore_nt_ps, prob_grid_doublescore_nt_ps, prob_grid_triplescore_nt_ps, prob_grid_bullscore_nt_ps, prob_grid_normalscore_t, prob_grid_singlescore_t, prob_grid_doublescore_t, prob_grid_triplescore_t, prob_grid_bullscore_t)\n",
    "\n",
    "    #### data for player A ####\n",
    "    num_aiming_location_pw, prob_normalscore_nt_pw, prob_doublescore_dic_nt_pw, prob_DB_nt_pw, prob_bust_dic_nt_pw, prob_notbust_dic_nt_pw, prob_normalscore_t, prob_doublescore_dic_t, prob_DB_t, prob_bust_dic_t, prob_notbust_dic_t = h.init_probabilities(aiming_grid, prob_grid_normalscore_nt_pw, prob_grid_doublescore_nt_pw, prob_grid_bullscore_nt_pw, prob_grid_normalscore_t, prob_grid_doublescore_t, prob_grid_bullscore_t)\n",
    "\n",
    "    param_pw = {}    \n",
    "\n",
    "    prob_normalscore_tensor_nt_pw = torch.from_numpy(prob_normalscore_nt_pw)\n",
    "    prob_normalscore_tensor_t = torch.from_numpy(prob_normalscore_t)\n",
    "    param_pw['prob_normalscore_tensor_nt'] = prob_normalscore_tensor_nt_pw\n",
    "    param_pw['prob_normalscore_tensor_t'] = prob_normalscore_tensor_t\n",
    "    param_pw['prob_doublescore_dic_nt'] = prob_doublescore_dic_nt_pw\n",
    "    param_pw['prob_doublescore_dic_t'] = prob_doublescore_dic_t\n",
    "    param_pw['prob_DB_nt'] = prob_DB_nt_pw\n",
    "    param_pw['prob_DB_t'] = prob_DB_t\n",
    "    param_pw['prob_bust_dic_nt'] = prob_bust_dic_nt_pw\n",
    "    param_pw['prob_bust_dic_t'] = prob_bust_dic_t\n",
    "\n",
    "    #### data for player B ####\n",
    "    num_aiming_location_ps, prob_normalscore_nt_ps, prob_doublescore_dic_nt_ps, prob_DB_nt_ps, prob_bust_dic_nt_ps, prob_notbust_dic_nt_ps, prob_normalscore_t, prob_doublescore_dic_t, prob_DB_t, prob_bust_dic_t, prob_notbust_dic_t = h.init_probabilities(aiming_grid, prob_grid_normalscore_nt_ps, prob_grid_doublescore_nt_ps, prob_grid_bullscore_nt_ps, prob_grid_normalscore_t, prob_grid_doublescore_t, prob_grid_bullscore_t)\n",
    "\n",
    "    param_ps = {}    \n",
    "\n",
    "    prob_normalscore_tensor_nt_ps = torch.from_numpy(prob_normalscore_nt_ps)\n",
    "    prob_normalscore_tensor_t = torch.from_numpy(prob_normalscore_t)\n",
    "    param_ps['prob_normalscore_tensor_nt'] = prob_normalscore_tensor_nt_ps\n",
    "    param_ps['prob_normalscore_tensor_t'] = prob_normalscore_tensor_t\n",
    "    param_ps['prob_doublescore_dic_nt'] = prob_doublescore_dic_nt_ps\n",
    "    param_ps['prob_doublescore_dic_t'] = prob_doublescore_dic_t\n",
    "    param_ps['prob_DB_nt'] = prob_DB_nt_ps\n",
    "    param_ps['prob_DB_t'] = prob_DB_t\n",
    "    param_ps['prob_bust_dic_nt'] = prob_bust_dic_nt_ps\n",
    "    param_ps['prob_bust_dic_t'] = prob_bust_dic_t\n",
    "        \n",
    "    #### \n",
    "    iteration_round_limit_zsgtwoplayers = 5\n",
    "    iteration_relerror_limit_zsgtwoplayers = 10**-9\n",
    "    iteration_round_zsgtwoplayers = np.zeros((max_tokens+1,502,502), dtype=np.int8)\n",
    "\n",
    "    iteration_round_limit_singleplayer_policy = 20\n",
    "    iteration_relerror_limit_singleplayer_policy = 10**-9\n",
    "\n",
    "    value_pw = np.zeros((max_tokens+1,502,502))  # player A's winning probability when A throws at state [score_A, score_B]\n",
    "    value_ps = np.zeros((max_tokens+1,502,502))  # player A's winning probability when B throws at state [score_A, score_B]\n",
    "    value_win_pw = 1.0\n",
    "    value_win_ps = 0.0\n",
    "    num_iteration_record_pw = np.zeros((max_tokens+1,502,502), dtype=np.int8)\n",
    "    num_iteration_record_ps = np.zeros((max_tokens+1,502,502), dtype=np.int8)\n",
    "    ## values when optimizing A\n",
    "    value_pw_optW = value_pw.copy()\n",
    "    value_ps_optW = value_ps.copy()\n",
    "    ## values when optimizing B\n",
    "    value_pw_optS = value_pw.copy()\n",
    "    value_ps_optS = value_ps.copy()    \n",
    "\n",
    "    state_len_vector_pw = np.zeros(4, dtype=np.int32)\n",
    "    state_value_default  = [None]  \n",
    "    action_diff_pw  = [None]\n",
    "    value_relerror_pw = np.zeros(4)\n",
    "    for rt in [1,2,3]:\n",
    "        ## for rt=3: possible score_gained = 0\n",
    "        ## for rt=2: possible score_gained = 0,1,...,60\n",
    "        ## for rt=1: possible score_gained = 0,1,...,120\n",
    "        this_throw_state_len = fb.maxhitscore*(3-rt) + 1\n",
    "        state_value_default.append(np.ones((max_tokens+1,this_throw_state_len))*fb.largenumber)\n",
    "        action_diff_pw.append(np.ones((max_tokens+1,this_throw_state_len)))    \n",
    "    state_len_vector_ps = np.zeros(4, dtype=np.int32)\n",
    "    action_diff_ps = ft.copy_numberarray_container(action_diff_pw)\n",
    "    value_relerror_ps = np.zeros(4)\n",
    "\n",
    "    optimal_value_dic_pw = {}\n",
    "    optimal_action_index_dic_pw = {}\n",
    "    prob_turn_transit_dic_pw = {}\n",
    "    optimal_value_dic_ps = {} \n",
    "    optimal_action_index_dic_ps = {}\n",
    "    prob_turn_transit_dic_ps = {}\n",
    "\n",
    "    for score in range(2,502):\n",
    "        optimal_value_dic_pw[score] = {}\n",
    "        optimal_action_index_dic_pw[score] = {}\n",
    "        prob_turn_transit_dic_pw[score] = {}\n",
    "        optimal_value_dic_ps[score] = {}\n",
    "        optimal_action_index_dic_ps[score] = {}\n",
    "        prob_turn_transit_dic_ps[score] = {}\n",
    "\n",
    "    #### algorithm start ####\n",
    "    t_policy_improvement = 0\n",
    "    t_policy_evaluation = 0\n",
    "    t_other = 0\n",
    "    t1 = time.time()\n",
    "    for score_state_ps in range(2, game_begin_score_502):\n",
    "        t_scoreloop_begin = time.time()\n",
    "        print('stronger state:',score_state_ps,'time:',t_scoreloop_begin-t1)\n",
    "        score_state_list = []\n",
    "\n",
    "        ## fix player B score, loop through player A\n",
    "        for score_state_pw in range(2, game_begin_score_502):\n",
    "\n",
    "            for tokens_pw in range(0,max_tokens+1):\n",
    "                \n",
    "                score_state_list.append([tokens_pw, score_state_pw, score_state_ps])\n",
    "\n",
    "        ########     solve all states in turn [score_A, score_B]    ########\n",
    "        for [tokens_pw, score_state_pw, score_state_ps] in score_state_list:\n",
    "            #print('##### score_state [score_pa, score_pb] = {} ####'.format([score_state_pa, score_state_pb]))\n",
    "\n",
    "            ## initialize the starting policy:\n",
    "            ## player A\n",
    "            for rt in [1,2,3]:        \n",
    "                this_throw_state_len_pw = min(score_state_pw-2, fb.maxhitscore*(3-rt)) + 1\n",
    "                state_len_vector_pw[rt] = this_throw_state_len_pw\n",
    "            state_value_pw = ft.copy_numberarray_container(state_value_default)\n",
    "            if score_state_ps > 2:\n",
    "                state_action_pw = ft.copy_numberarray_container(optimal_action_index_dic_pw[score_state_pw][score_state_ps-1])\n",
    "                prob_turn_transit_pw = prob_turn_transit_dic_pw[score_state_pw][score_state_ps-1]\n",
    "            else:\n",
    "                state_action_pw = ft.copy_numberarray_container(dp_policy_dict_pw['optimal_action_index_dic'][score_state_pw])\n",
    "                prob_turn_transit_pw = dp_policy_dict_pw['prob_scorestate_transit'][tokens_pw][score_state_pw]\n",
    "            state_value_update_pw = ft.copy_numberarray_container(state_value_pw)\n",
    "            state_action_update_pw = ft.copy_numberarray_container(state_action_pw)\n",
    "\n",
    "\n",
    "            ## player B\n",
    "            for rt in [1,2,3]:        \n",
    "                this_throw_state_len_ps = min(score_state_ps-2, fb.maxhitscore*(3-rt)) + 1\n",
    "                state_len_vector_ps[rt] = this_throw_state_len_ps\n",
    "            state_value_ps = ft.copy_numberarray_container(state_value_default)\n",
    "            if score_state_pw > 2:\n",
    "                state_action_ps = ft.copy_numberarray_container(optimal_action_index_dic_ps[score_state_pw-1][score_state_ps])\n",
    "                prob_turn_transit_ps = prob_turn_transit_dic_ps[score_state_pw-1][score_state_ps]\n",
    "            else:\n",
    "                state_action_ps = ft.copy_numberarray_container(dp_policy_dict_ps['optimal_action_index_dic'][score_state_ps])\n",
    "                prob_turn_transit_ps = dp_policy_dict_ps['prob_scorestate_transit'][0][score_state_ps]\n",
    "            state_value_update_ps = ft.copy_numberarray_container(state_value_ps)\n",
    "            state_action_update_ps = ft.copy_numberarray_container(state_action_ps)\n",
    "            \n",
    "            \n",
    "            ## assemble variables\n",
    "            ## player A\n",
    "            param_pw['score_state'] = score_state_pw   \n",
    "            param_pw['token_state'] = tokens_pw   \n",
    "            param_pw['state_len_vector'] = state_len_vector_pw        \n",
    "            param_pw['state_action'] = state_action_pw\n",
    "            param_pw['state_value'] = state_value_pw\n",
    "            param_pw['state_action_update'] = state_action_update_pw\n",
    "            param_pw['state_value_update'] = state_value_update_pw\n",
    "            param_pw['action_diff'] = action_diff_pw\n",
    "            param_pw['value_relerror'] = value_relerror_pw   \n",
    "            ## maximize player A's win_prob\n",
    "            param_pw['flag_max'] = True\n",
    "            param_pw['next_turn_value'] = value_ps[:,:,score_state_ps] ## player B throws in next turn\n",
    "            param_pw['game_end_value'] = value_win_pw ## end game state A win\n",
    "            \n",
    "            ## player B\n",
    "            param_ps['score_state'] = score_state_ps  \n",
    "            param_ps['token_state'] = 0     \n",
    "            param_ps['state_len_vector'] = state_len_vector_ps\n",
    "            param_ps['state_action'] = state_action_ps\n",
    "            param_ps['state_value'] = state_value_ps\n",
    "            param_ps['state_action_update'] = state_action_update_ps\n",
    "            param_ps['state_value_update'] = state_value_update_ps\n",
    "            param_ps['action_diff'] = action_diff_ps\n",
    "            param_ps['value_relerror'] = value_relerror_ps    \n",
    "            ## maximize player B's win_prob = minimize player A's win_prob\n",
    "            param_ps['flag_max'] = False\n",
    "            param_ps['next_turn_value'] = value_pw[:,score_state_pw,:] ## player A throws in next turn\n",
    "            param_ps['game_end_value'] = value_win_ps ## end game state B win     \n",
    "            \n",
    "            ## optimize A and B iteratively\n",
    "            for round_index_zsgtwoplayers in range(iteration_round_limit_zsgtwoplayers):\n",
    "                ## print('## optimize two players round = {} ##'.format(round_index_zsgtwoplayers))\n",
    "                ## iterate at least once for each player\n",
    "                \n",
    "                #### optimize A policy ####\n",
    "                value_pw_state_old = value_pw[tokens_pw, score_state_pw,score_state_ps] ## starting value 0\n",
    "                value_ps_state_old = value_ps[tokens_pw, score_state_pw,score_state_ps] ## starting value 0                \n",
    "                for round_index in range(iteration_round_limit_singleplayer_policy):                    \n",
    "                    \n",
    "                    ## policy evaluation\n",
    "                    tpe1 = time.time()                \n",
    "                    ## use the initial prob_turn_transit_pa value for round_index=0\n",
    "                    if (round_index >=0):\n",
    "                        prob_turn_transit_pw = h.solve_turn_transit_probability_fast_token(score_state=score_state_pw,state_action=state_action_pw,available_tokens=tokens_pw,prob_grid_normalscore_nt=prob_grid_normalscore_nt_pw,prob_grid_doublescore_nt=prob_grid_doublescore_nt_pw,prob_grid_bullscore_nt=prob_grid_bullscore_nt_pw,prob_bust_dic_nt=prob_bust_dic_nt_pw,prob_grid_normalscore_t=prob_grid_normalscore_t,prob_grid_doublescore_t=prob_grid_doublescore_t,prob_grid_bullscore_t=prob_grid_bullscore_t,prob_bust_dic_t=prob_bust_dic_t)\n",
    "                    ## player B is fixed, use stored value\n",
    "                    [value_state_pw, value_state_ps] = zsg_policy_evaluation_tokens(value_pw, value_ps, tokens_pw, score_state_pw, score_state_ps, prob_turn_transit_pw, prob_turn_transit_ps)\n",
    "                    value_pw[tokens_pw, score_state_pw, score_state_ps] = value_state_pw\n",
    "                    value_ps[tokens_pw, score_state_pw, score_state_ps] = value_state_ps\n",
    "                    tpe2 = time.time()\n",
    "                    t_policy_evaluation += (tpe2-tpe1) \n",
    "\n",
    "                    #### policy improvement for player A ####\n",
    "                    tpi1 = time.time()\n",
    "                    param_pw['round_index'] = round_index\n",
    "                    [max_action_diff_pw, max_value_relerror_pw] = zsg_policy_improvement_tokens(param_pw)\n",
    "                    tpi2 = time.time()\n",
    "                    t_policy_improvement += (tpi2 - tpi1)\n",
    "                    if (max_action_diff_pw < 1):\n",
    "                        break    \n",
    "                    if (max_value_relerror_pw < iteration_relerror_limit_singleplayer_policy):\n",
    "                        break\n",
    "        \n",
    "                optimal_action_index_dic_pw[score_state_pw][score_state_ps] = state_action_pw\n",
    "                optimal_value_dic_pw[score_state_pw][score_state_ps] = state_value_pw\n",
    "                prob_turn_transit_dic_pw[score_state_pw][score_state_ps] = prob_turn_transit_pw\n",
    "                num_iteration_record_pw[tokens_pw,score_state_pw, score_state_ps] = round_index + 1\n",
    "                #### done optimize player A\n",
    "                \n",
    "                ## check optimality\n",
    "                value_pw_optW[tokens_pw, score_state_pw,score_state_ps] = value_pw[tokens_pw, score_state_pw,score_state_ps]\n",
    "                value_ps_optW[tokens_pw, score_state_pw,score_state_ps] = value_ps[tokens_pw, score_state_pw,score_state_pw]\n",
    "                max_zsgvalue_relerror = max([np.abs(value_pw_state_old-value_pw[tokens_pw, score_state_pw,score_state_ps]), np.abs(value_ps_state_old-value_ps[tokens_pw, score_state_pw,score_state_ps])])\n",
    "                #print('A:max_zsgvalue_relerror={}'.format(max_zsgvalue_relerror))      \n",
    "                if (max_zsgvalue_relerror < iteration_relerror_limit_zsgtwoplayers):\n",
    "                    break\n",
    "\n",
    "\n",
    "                #### optimize B policy ####\n",
    "                value_pw_state_old = value_pw[tokens_pw, score_state_pw,score_state_ps] ## starting value 0\n",
    "                value_ps_state_old = value_ps[tokens_pw, score_state_pw,score_state_ps] ## starting value 0                \n",
    "                for round_index in range(iteration_round_limit_singleplayer_policy):                    \n",
    "                    \n",
    "                    ## policy evaluation\n",
    "                    tpe1 = time.time()\n",
    "                    ## player A is fixed, only need to compute once\n",
    "                    if (round_index >=0):\n",
    "                        prob_turn_transit_ps = h.solve_turn_transit_probability_fast_token(score_state=score_state_ps,state_action=state_action_ps,available_tokens=0,prob_grid_normalscore_nt=prob_grid_normalscore_nt_ps,prob_grid_doublescore_nt=prob_grid_doublescore_nt_ps,prob_grid_bullscore_nt=prob_grid_bullscore_nt_ps,prob_bust_dic_nt=prob_bust_dic_nt_ps,prob_grid_normalscore_t=prob_grid_normalscore_t,prob_grid_doublescore_t=prob_grid_doublescore_t,prob_grid_bullscore_t=prob_grid_bullscore_t,prob_bust_dic_t=prob_bust_dic_t)\n",
    "                    [value_state_pw, value_state_ps] = zsg_policy_evaluation_tokens(value_pw, value_ps, tokens_pw, score_state_pw, score_state_ps, prob_turn_transit_pw, prob_turn_transit_ps)\n",
    "                    value_pw[tokens_pw, score_state_pw, score_state_ps] = value_state_pw\n",
    "                    value_ps[tokens_pw, score_state_pw, score_state_ps] = value_state_ps\n",
    "                    tpe2 = time.time()\n",
    "                    t_policy_evaluation += (tpe2-tpe1) \n",
    "\n",
    "                    #### policy improvement for player B ####\n",
    "                    tpi1 = time.time()\n",
    "                    param_ps['round_index'] = round_index\n",
    "                    [max_action_diff_ps, max_value_relerror_ps] = zsg_policy_improvement_tokens(param_ps)\n",
    "                    tpi2 = time.time()\n",
    "                    t_policy_improvement += (tpi2 - tpi1)\n",
    "                    if (max_action_diff_ps < 1):\n",
    "                        break    \n",
    "                    if (max_value_relerror_ps < iteration_relerror_limit_singleplayer_policy):\n",
    "                        break\n",
    "        \n",
    "                optimal_action_index_dic_ps[score_state_pw][score_state_ps] = state_action_ps\n",
    "                optimal_value_dic_ps[score_state_pw][score_state_ps] = state_value_ps\n",
    "                prob_turn_transit_dic_ps[score_state_pw][score_state_ps] = prob_turn_transit_ps\n",
    "                num_iteration_record_ps[tokens_pw, score_state_pw, score_state_ps] = round_index + 1\n",
    "                #### done optimize player B\n",
    "        \n",
    "                ## check optimality\n",
    "                value_pw_optS[tokens_pw, score_state_pw,score_state_ps] = value_pw[tokens_pw, score_state_pw,score_state_ps]\n",
    "                value_ps_optS[tokens_pw, score_state_pw,score_state_ps] = value_ps[tokens_pw, score_state_pw,score_state_ps]\n",
    "                max_zsgvalue_relerror = max([np.abs(value_pw_state_old-value_pw[tokens_pw, score_state_pw,score_state_ps]), np.abs(value_ps_state_old-value_ps[tokens_pw, score_state_pw,score_state_ps])])\n",
    "                #print('B:max_zsgvalue_relerror={}'.format(max_zsgvalue_relerror))\n",
    "                if (max_zsgvalue_relerror < iteration_relerror_limit_zsgtwoplayers):\n",
    "                    break\n",
    "            \n",
    "            #### done optimize A and B iteratively\n",
    "            value_pw[tokens_pw,score_state_pw,score_state_ps] = 0.5*(value_pw_optW[tokens_pw, score_state_pw,score_state_ps]+value_pw_optS[tokens_pw, score_state_pw,score_state_ps])\n",
    "            value_ps[tokens_pw,score_state_pw,score_state_ps] = 0.5*(value_ps_optW[tokens_pw, score_state_pw,score_state_ps]+value_ps_optS[tokens_pw, score_state_pw,score_state_ps])\n",
    "            iteration_round_zsgtwoplayers[tokens_pw,score_state_pw,score_state_ps] = round_index_zsgtwoplayers + 1\n",
    "            #print('optimize A and B iteratively in time={} seconds'.format(time.time()-t_opt_twoplayers_begin))\n",
    "        \n",
    "        #### finish a column        \n",
    "        #if (score_state_pb%20==0 or score_state_pb==2):\n",
    "        #    print('#### score_state_pb={}, time={}'.format(score_state_pb, time.time()-t_scoreloop_begin))\n",
    "\n",
    "    ## computation is done\n",
    "    t2 = time.time()\n",
    "    print('solve_zsg_opt_{}e{}_fix_{}e{} in {} seconds'.format(name_ps,epsilon_ps, name_pw, epsilon_pw, t2-t1))\n",
    "    print('t_policy_evaluation  = {} seconds'.format(t_policy_evaluation))\n",
    "    print('t_policy_improvement = {} seconds'.format(t_policy_improvement))\n",
    "    print('t_other = {} seconds'.format(t_other))   \n",
    "    #print('value_pa {} '.format(value_pa))\n",
    "    #print('value_pb {} '.format(value_pb))\n",
    "        \n",
    "    \n",
    "    result_dic = {'info':info, 'optimal_action_index_dic_pw':optimal_action_index_dic_pw, 'optimal_action_index_dic_ps':optimal_action_index_dic_ps, 'value_pw':value_pw, 'value_ps':value_ps, 'value_pw_optW':value_pw_optW, 'value_pw_optS':value_pw_optS, 'value_ps_optW':value_ps_optW, 'value_ps_optS':value_ps_optS,  'optimal_value_dic_pw':optimal_value_dic_pw, 'optimal_value_dic_ps':optimal_value_dic_ps, 'iteration_round_zsgtwoplayers':iteration_round_zsgtwoplayers, 'num_iteration_record_pw':num_iteration_record_pw, 'num_iteration_record_ps':num_iteration_record_ps}\n",
    "    if (result_dir is not None):\n",
    "        ft.dump_pickle(result_filename, result_dic)\n",
    "        print('save {}'.format(result_filename))\n",
    "        ft.dump_pickle(result_value_filename, {'info':info, 'value_pw':value_pw, 'value_ps':value_ps, 'iteration_round_zsgtwoplayers':iteration_round_zsgtwoplayers})\n",
    "        print('save {}'.format(result_value_filename))\n",
    "        return 'save'\n",
    "    else:\n",
    "        return result_dic\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "solve_zsg_optboth(name_pw, name_ps, epsilon_pw, epsilon_ps, data_parameter_dir=fb.data_parameter_dir, dp_policy_folder='result', result_dir='result', postfix='', gpu_device=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "info = 'W_{}e{}_S_{}e{}_optboth'.format(name_pw, epsilon_pw, name_ps, epsilon_ps)\n",
    "print(info)\n",
    "##\n",
    "if result_dir is not None:\n",
    "    if not os.path.isdir(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "    result_filename = result_dir + '/zsg_W_{}e{}_S_{}e{}_{}_optboth.pkl'.format(name_pw, epsilon_pw, name_ps, epsilon_ps, postfix)\n",
    "    result_value_filename = result_dir + '/zsg_value_W_{}e{}_S_{}e{}_{}_optboth.pkl'.format(name_pw, epsilon_pw, name_ps, epsilon_ps, postfix)\n",
    "\n",
    "\n",
    "max_tokens = 9\n",
    "#player A: pa throw first\n",
    "#player B: pb throw after player A, policy is fixed as ns\n",
    "print('player W is {} e{} and player S is {} e{}'.format(name_pw, epsilon_pw, name_ps, epsilon_ps))\n",
    "print('optimize both players')\n",
    "\n",
    "[aiming_grid, prob_grid_normalscore_nt_pw, prob_grid_singlescore_nt_pw, prob_grid_doublescore_nt_pw, prob_grid_triplescore_nt_pw, prob_grid_bullscore_nt_pw] = h.load_aiming_grid(name_pw, epsilon=epsilon_pw, data_parameter_dir=data_parameter_dir, grid_version='custom_no_tokens')\n",
    "[aiming_grid, prob_grid_normalscore_nt_ps, prob_grid_singlescore_nt_ps, prob_grid_doublescore_nt_ps, prob_grid_triplescore_nt_ps, prob_grid_bullscore_nt_ps] = h.load_aiming_grid(name_ps, epsilon=epsilon_ps, data_parameter_dir=data_parameter_dir, grid_version='custom_no_tokens')\n",
    "[aiming_grid, prob_grid_normalscore_t, prob_grid_singlescore_t, prob_grid_doublescore_t, prob_grid_triplescore_t, prob_grid_bullscore_t] = h.load_aiming_grid('t', data_parameter_dir=data_parameter_dir, grid_version='custom_tokens')\n",
    "\n",
    "## use single player game as the fixed policy    \n",
    "dp_policy_dict_pw, dp_policy_dict_ps = load_ns_policy_dicts(name_pw,name_ps,epsilon_pw,epsilon_ps,dp_policy_folder,aiming_grid, prob_grid_normalscore_nt_pw, prob_grid_singlescore_nt_pw, prob_grid_doublescore_nt_pw, prob_grid_triplescore_nt_pw, prob_grid_bullscore_nt_pw, prob_grid_normalscore_nt_ps, prob_grid_singlescore_nt_ps, prob_grid_doublescore_nt_ps, prob_grid_triplescore_nt_ps, prob_grid_bullscore_nt_ps, prob_grid_normalscore_t, prob_grid_singlescore_t, prob_grid_doublescore_t, prob_grid_triplescore_t, prob_grid_bullscore_t)\n",
    "\n",
    "#### data for player A ####\n",
    "num_aiming_location_pw, prob_normalscore_nt_pw, prob_doublescore_dic_nt_pw, prob_DB_nt_pw, prob_bust_dic_nt_pw, prob_notbust_dic_nt_pw, prob_normalscore_t, prob_doublescore_dic_t, prob_DB_t, prob_bust_dic_t, prob_notbust_dic_t = h.init_probabilities(aiming_grid, prob_grid_normalscore_nt_pw, prob_grid_doublescore_nt_pw, prob_grid_bullscore_nt_pw, prob_grid_normalscore_t, prob_grid_doublescore_t, prob_grid_bullscore_t)\n",
    "\n",
    "param_pw = {}    \n",
    "\n",
    "prob_normalscore_tensor_nt_pw = torch.from_numpy(prob_normalscore_nt_pw)\n",
    "prob_normalscore_tensor_t = torch.from_numpy(prob_normalscore_t)\n",
    "param_pw['prob_normalscore_tensor_nt'] = prob_normalscore_tensor_nt_pw\n",
    "param_pw['prob_normalscore_tensor_t'] = prob_normalscore_tensor_t\n",
    "param_pw['prob_doublescore_dic_nt'] = prob_doublescore_dic_nt_pw\n",
    "param_pw['prob_doublescore_dic_t'] = prob_doublescore_dic_t\n",
    "param_pw['prob_DB_nt'] = prob_DB_nt_pw\n",
    "param_pw['prob_DB_t'] = prob_DB_t\n",
    "param_pw['prob_bust_dic_nt'] = prob_bust_dic_nt_pw\n",
    "param_pw['prob_bust_dic_t'] = prob_bust_dic_t\n",
    "\n",
    "#### data for player B ####\n",
    "num_aiming_location_ps, prob_normalscore_nt_ps, prob_doublescore_dic_nt_ps, prob_DB_nt_ps, prob_bust_dic_nt_ps, prob_notbust_dic_nt_ps, prob_normalscore_t, prob_doublescore_dic_t, prob_DB_t, prob_bust_dic_t, prob_notbust_dic_t = h.init_probabilities(aiming_grid, prob_grid_normalscore_nt_ps, prob_grid_doublescore_nt_ps, prob_grid_bullscore_nt_ps, prob_grid_normalscore_t, prob_grid_doublescore_t, prob_grid_bullscore_t)\n",
    "\n",
    "param_ps = {}    \n",
    "\n",
    "prob_normalscore_tensor_nt_ps = torch.from_numpy(prob_normalscore_nt_ps)\n",
    "prob_normalscore_tensor_t = torch.from_numpy(prob_normalscore_t)\n",
    "param_ps['prob_normalscore_tensor_nt'] = prob_normalscore_tensor_nt_ps\n",
    "param_ps['prob_normalscore_tensor_t'] = prob_normalscore_tensor_t\n",
    "param_ps['prob_doublescore_dic_nt'] = prob_doublescore_dic_nt_ps\n",
    "param_ps['prob_doublescore_dic_t'] = prob_doublescore_dic_t\n",
    "param_ps['prob_DB_nt'] = prob_DB_nt_ps\n",
    "param_ps['prob_DB_t'] = prob_DB_t\n",
    "param_ps['prob_bust_dic_nt'] = prob_bust_dic_nt_ps\n",
    "param_ps['prob_bust_dic_t'] = prob_bust_dic_t\n",
    "    \n",
    "#### \n",
    "iteration_round_limit_zsgtwoplayers = 5\n",
    "iteration_relerror_limit_zsgtwoplayers = 10**-9\n",
    "iteration_round_zsgtwoplayers = np.zeros((max_tokens+1,502,502), dtype=np.int8)\n",
    "\n",
    "iteration_round_limit_singleplayer_policy = 20\n",
    "iteration_relerror_limit_singleplayer_policy = 10**-9\n",
    "\n",
    "value_pw = np.zeros((max_tokens+1,502,502))  # player A's winning probability when A throws at state [score_A, score_B]\n",
    "value_ps = np.zeros((max_tokens+1,502,502))  # player A's winning probability when B throws at state [score_A, score_B]\n",
    "value_win_pw = 1.0\n",
    "value_win_ps = 0.0\n",
    "num_iteration_record_pw = np.zeros((max_tokens+1,502,502), dtype=np.int8)\n",
    "num_iteration_record_ps = np.zeros((max_tokens+1,502,502), dtype=np.int8)\n",
    "## values when optimizing A\n",
    "value_pw_optW = value_pw.copy()\n",
    "value_ps_optW = value_ps.copy()\n",
    "## values when optimizing B\n",
    "value_pw_optS = value_pw.copy()\n",
    "value_ps_optS = value_ps.copy()    \n",
    "\n",
    "state_len_vector_pw = np.zeros(4, dtype=np.int32)\n",
    "state_value_default  = [None]  \n",
    "action_diff_pw  = [None]\n",
    "value_relerror_pw = np.zeros(4)\n",
    "for rt in [1,2,3]:\n",
    "    ## for rt=3: possible score_gained = 0\n",
    "    ## for rt=2: possible score_gained = 0,1,...,60\n",
    "    ## for rt=1: possible score_gained = 0,1,...,120\n",
    "    this_throw_state_len = fb.maxhitscore*(3-rt) + 1\n",
    "    state_value_default.append(np.ones((max_tokens+1,this_throw_state_len))*fb.largenumber)\n",
    "    action_diff_pw.append(np.ones((max_tokens+1,this_throw_state_len)))    \n",
    "state_len_vector_ps = np.zeros(4, dtype=np.int32)\n",
    "action_diff_ps = ft.copy_numberarray_container(action_diff_pw)\n",
    "value_relerror_ps = np.zeros(4)\n",
    "\n",
    "optimal_value_dic_pw = {}\n",
    "optimal_action_index_dic_pw = {}\n",
    "prob_turn_transit_dic_pw = {}\n",
    "optimal_value_dic_ps = {} \n",
    "optimal_action_index_dic_ps = {}\n",
    "prob_turn_transit_dic_ps = {}\n",
    "\n",
    "for score in range(2,502):\n",
    "    optimal_value_dic_pw[score] = {}\n",
    "    optimal_action_index_dic_pw[score] = {}\n",
    "    prob_turn_transit_dic_pw[score] = {}\n",
    "    optimal_value_dic_ps[score] = {}\n",
    "    optimal_action_index_dic_ps[score] = {}\n",
    "    prob_turn_transit_dic_ps[score] = {}\n",
    "\n",
    "#### algorithm start ####\n",
    "t_policy_improvement = 0\n",
    "t_policy_evaluation = 0\n",
    "t_other = 0\n",
    "t1 = time.time()\n",
    "for score_state_ps in range(2, game_begin_score_502):\n",
    "    t_scoreloop_begin = time.time()\n",
    "    print('stronger state:',score_state_ps,'time:',t_scoreloop_begin-t1)\n",
    "    score_state_list = []\n",
    "\n",
    "    ## fix player B score, loop through player A\n",
    "    for score_state_pw in range(2, game_begin_score_502):\n",
    "\n",
    "        for tokens_pw in range(0,max_tokens+1):\n",
    "            \n",
    "            score_state_list.append([tokens_pw, score_state_pw, score_state_ps])\n",
    "\n",
    "    ########     solve all states in turn [score_A, score_B]    ########\n",
    "    for [tokens_pw, score_state_pw, score_state_ps] in score_state_list:\n",
    "        #print('##### score_state [score_pa, score_pb] = {} ####'.format([score_state_pa, score_state_pb]))\n",
    "\n",
    "        ## initialize the starting policy:\n",
    "        ## player A\n",
    "        for rt in [1,2,3]:        \n",
    "            this_throw_state_len_pw = min(score_state_pw-2, fb.maxhitscore*(3-rt)) + 1\n",
    "            state_len_vector_pw[rt] = this_throw_state_len_pw\n",
    "        state_value_pw = ft.copy_numberarray_container(state_value_default)\n",
    "        if score_state_ps > 2:\n",
    "            state_action_pw = ft.copy_numberarray_container(optimal_action_index_dic_pw[score_state_pw][score_state_ps-1])\n",
    "            prob_turn_transit_pw = prob_turn_transit_dic_pw[score_state_pw][score_state_ps-1]\n",
    "        else:\n",
    "            state_action_pw = ft.copy_numberarray_container(dp_policy_dict_pw['optimal_action_index_dic'][score_state_pw])\n",
    "            prob_turn_transit_pw = dp_policy_dict_pw['prob_scorestate_transit'][tokens_pw][score_state_pw]\n",
    "        state_value_update_pw = ft.copy_numberarray_container(state_value_pw)\n",
    "        state_action_update_pw = ft.copy_numberarray_container(state_action_pw)\n",
    "\n",
    "\n",
    "        ## player B\n",
    "        for rt in [1,2,3]:        \n",
    "            this_throw_state_len_ps = min(score_state_ps-2, fb.maxhitscore*(3-rt)) + 1\n",
    "            state_len_vector_ps[rt] = this_throw_state_len_ps\n",
    "        state_value_ps = ft.copy_numberarray_container(state_value_default)\n",
    "        if score_state_pw > 2:\n",
    "            state_action_ps = ft.copy_numberarray_container(optimal_action_index_dic_ps[score_state_pw-1][score_state_ps])\n",
    "            prob_turn_transit_ps = prob_turn_transit_dic_ps[score_state_pw-1][score_state_ps]\n",
    "        else:\n",
    "            state_action_ps = ft.copy_numberarray_container(dp_policy_dict_ps['optimal_action_index_dic'][score_state_ps])\n",
    "            prob_turn_transit_ps = dp_policy_dict_ps['prob_scorestate_transit'][0][score_state_ps]\n",
    "        state_value_update_ps = ft.copy_numberarray_container(state_value_ps)\n",
    "        state_action_update_ps = ft.copy_numberarray_container(state_action_ps)\n",
    "        \n",
    "        \n",
    "        ## assemble variables\n",
    "        ## player A\n",
    "        param_pw['score_state'] = score_state_pw   \n",
    "        param_pw['token_state'] = tokens_pw   \n",
    "        param_pw['state_len_vector'] = state_len_vector_pw        \n",
    "        param_pw['state_action'] = state_action_pw\n",
    "        param_pw['state_value'] = state_value_pw\n",
    "        param_pw['state_action_update'] = state_action_update_pw\n",
    "        param_pw['state_value_update'] = state_value_update_pw\n",
    "        param_pw['action_diff'] = action_diff_pw\n",
    "        param_pw['value_relerror'] = value_relerror_pw   \n",
    "        ## maximize player A's win_prob\n",
    "        param_pw['flag_max'] = True\n",
    "        param_pw['next_turn_value'] = value_ps[:,:,score_state_ps] ## player B throws in next turn\n",
    "        param_pw['game_end_value'] = value_win_pw ## end game state A win\n",
    "        \n",
    "        ## player B\n",
    "        param_ps['score_state'] = score_state_ps  \n",
    "        param_ps['token_state'] = 0     \n",
    "        param_ps['state_len_vector'] = state_len_vector_ps\n",
    "        param_ps['state_action'] = state_action_ps\n",
    "        param_ps['state_value'] = state_value_ps\n",
    "        param_ps['state_action_update'] = state_action_update_ps\n",
    "        param_ps['state_value_update'] = state_value_update_ps\n",
    "        param_ps['action_diff'] = action_diff_ps\n",
    "        param_ps['value_relerror'] = value_relerror_ps    \n",
    "        ## maximize player B's win_prob = minimize player A's win_prob\n",
    "        param_ps['flag_max'] = False\n",
    "        param_ps['next_turn_value'] = value_pw[:,score_state_pw,:] ## player A throws in next turn\n",
    "        param_ps['game_end_value'] = value_win_ps ## end game state B win     \n",
    "        \n",
    "        ## optimize A and B iteratively\n",
    "        for round_index_zsgtwoplayers in range(iteration_round_limit_zsgtwoplayers):\n",
    "            ## print('## optimize two players round = {} ##'.format(round_index_zsgtwoplayers))\n",
    "            ## iterate at least once for each player\n",
    "            \n",
    "            #### optimize A policy ####\n",
    "            value_pw_state_old = value_pw[tokens_pw, score_state_pw,score_state_ps] ## starting value 0\n",
    "            value_ps_state_old = value_ps[tokens_pw, score_state_pw,score_state_ps] ## starting value 0                \n",
    "            for round_index in range(iteration_round_limit_singleplayer_policy):                    \n",
    "                \n",
    "                ## policy evaluation\n",
    "                tpe1 = time.time()                \n",
    "                ## use the initial prob_turn_transit_pa value for round_index=0\n",
    "                if (round_index >=0):\n",
    "                    prob_turn_transit_pw = h.solve_turn_transit_probability_fast_token(score_state=score_state_pw,state_action=state_action_pw,available_tokens=tokens_pw,prob_grid_normalscore_nt=prob_grid_normalscore_nt_pw,prob_grid_doublescore_nt=prob_grid_doublescore_nt_pw,prob_grid_bullscore_nt=prob_grid_bullscore_nt_pw,prob_bust_dic_nt=prob_bust_dic_nt_pw,prob_grid_normalscore_t=prob_grid_normalscore_t,prob_grid_doublescore_t=prob_grid_doublescore_t,prob_grid_bullscore_t=prob_grid_bullscore_t,prob_bust_dic_t=prob_bust_dic_t)\n",
    "                ## player B is fixed, use stored value\n",
    "                [value_state_pw, value_state_ps] = zsg_policy_evaluation_tokens(value_pw, value_ps, tokens_pw, score_state_pw, score_state_ps, prob_turn_transit_pw, prob_turn_transit_ps)\n",
    "                value_pw[tokens_pw, score_state_pw, score_state_ps] = value_state_pw\n",
    "                value_ps[tokens_pw, score_state_pw, score_state_ps] = value_state_ps\n",
    "                tpe2 = time.time()\n",
    "                t_policy_evaluation += (tpe2-tpe1) \n",
    "\n",
    "                #### policy improvement for player A ####\n",
    "                tpi1 = time.time()\n",
    "                param_pw['round_index'] = round_index\n",
    "                [max_action_diff_pw, max_value_relerror_pw] = zsg_policy_improvement_tokens(param_pw)\n",
    "                tpi2 = time.time()\n",
    "                t_policy_improvement += (tpi2 - tpi1)\n",
    "                if (max_action_diff_pw < 1):\n",
    "                    break    \n",
    "                if (max_value_relerror_pw < iteration_relerror_limit_singleplayer_policy):\n",
    "                    break\n",
    "    \n",
    "            optimal_action_index_dic_pw[score_state_pw][score_state_ps] = state_action_pw\n",
    "            optimal_value_dic_pw[score_state_pw][score_state_ps] = state_value_pw\n",
    "            prob_turn_transit_dic_pw[score_state_pw][score_state_ps] = prob_turn_transit_pw\n",
    "            num_iteration_record_pw[tokens_pw,score_state_pw, score_state_ps] = round_index + 1\n",
    "            #### done optimize player A\n",
    "            \n",
    "            ## check optimality\n",
    "            value_pw_optW[tokens_pw, score_state_pw,score_state_ps] = value_pw[tokens_pw, score_state_pw,score_state_ps]\n",
    "            value_ps_optW[tokens_pw, score_state_pw,score_state_ps] = value_ps[tokens_pw, score_state_pw,score_state_pw]\n",
    "            max_zsgvalue_relerror = max([np.abs(value_pw_state_old-value_pw[tokens_pw, score_state_pw,score_state_ps]), np.abs(value_ps_state_old-value_ps[tokens_pw, score_state_pw,score_state_ps])])\n",
    "            #print('A:max_zsgvalue_relerror={}'.format(max_zsgvalue_relerror))      \n",
    "            if (max_zsgvalue_relerror < iteration_relerror_limit_zsgtwoplayers):\n",
    "                break\n",
    "\n",
    "\n",
    "            #### optimize B policy ####\n",
    "            value_pw_state_old = value_pw[tokens_pw, score_state_pw,score_state_ps] ## starting value 0\n",
    "            value_ps_state_old = value_ps[tokens_pw, score_state_pw,score_state_ps] ## starting value 0                \n",
    "            for round_index in range(iteration_round_limit_singleplayer_policy):                    \n",
    "                \n",
    "                ## policy evaluation\n",
    "                tpe1 = time.time()\n",
    "                ## player A is fixed, only need to compute once\n",
    "                if (round_index >=0):\n",
    "                    prob_turn_transit_ps = h.solve_turn_transit_probability_fast_token(score_state=score_state_ps,state_action=state_action_ps,available_tokens=0,prob_grid_normalscore_nt=prob_grid_normalscore_nt_ps,prob_grid_doublescore_nt=prob_grid_doublescore_nt_ps,prob_grid_bullscore_nt=prob_grid_bullscore_nt_ps,prob_bust_dic_nt=prob_bust_dic_nt_ps,prob_grid_normalscore_t=prob_grid_normalscore_t,prob_grid_doublescore_t=prob_grid_doublescore_t,prob_grid_bullscore_t=prob_grid_bullscore_t,prob_bust_dic_t=prob_bust_dic_t)\n",
    "                [value_state_pw, value_state_ps] = zsg_policy_evaluation_tokens(value_pw, value_ps, tokens_pw, score_state_pw, score_state_ps, prob_turn_transit_pw, prob_turn_transit_ps)\n",
    "                value_pw[tokens_pw, score_state_pw, score_state_ps] = value_state_pw\n",
    "                value_ps[tokens_pw, score_state_pw, score_state_ps] = value_state_ps\n",
    "                tpe2 = time.time()\n",
    "                t_policy_evaluation += (tpe2-tpe1) \n",
    "\n",
    "                #### policy improvement for player B ####\n",
    "                tpi1 = time.time()\n",
    "                param_ps['round_index'] = round_index\n",
    "                [max_action_diff_ps, max_value_relerror_ps] = zsg_policy_improvement_tokens(param_ps)\n",
    "                tpi2 = time.time()\n",
    "                t_policy_improvement += (tpi2 - tpi1)\n",
    "                if (max_action_diff_ps < 1):\n",
    "                    break    \n",
    "                if (max_value_relerror_ps < iteration_relerror_limit_singleplayer_policy):\n",
    "                    break\n",
    "    \n",
    "            optimal_action_index_dic_ps[score_state_pw][score_state_ps] = state_action_ps\n",
    "            optimal_value_dic_ps[score_state_pw][score_state_ps] = state_value_ps\n",
    "            prob_turn_transit_dic_ps[score_state_pw][score_state_ps] = prob_turn_transit_ps\n",
    "            num_iteration_record_ps[tokens_pw, score_state_pw, score_state_ps] = round_index + 1\n",
    "            #### done optimize player B\n",
    "    \n",
    "            ## check optimality\n",
    "            value_pw_optS[tokens_pw, score_state_pw,score_state_ps] = value_pw[tokens_pw, score_state_pw,score_state_ps]\n",
    "            value_ps_optS[tokens_pw, score_state_pw,score_state_ps] = value_ps[tokens_pw, score_state_pw,score_state_ps]\n",
    "            max_zsgvalue_relerror = max([np.abs(value_pw_state_old-value_pw[tokens_pw, score_state_pw,score_state_ps]), np.abs(value_ps_state_old-value_ps[tokens_pw, score_state_pw,score_state_ps])])\n",
    "            #print('B:max_zsgvalue_relerror={}'.format(max_zsgvalue_relerror))\n",
    "            if (max_zsgvalue_relerror < iteration_relerror_limit_zsgtwoplayers):\n",
    "                break\n",
    "        \n",
    "        #### done optimize A and B iteratively\n",
    "        value_pw[tokens_pw,score_state_pw,score_state_ps] = 0.5*(value_pw_optW[tokens_pw, score_state_pw,score_state_ps]+value_pw_optS[tokens_pw, score_state_pw,score_state_ps])\n",
    "        value_ps[tokens_pw,score_state_pw,score_state_ps] = 0.5*(value_ps_optW[tokens_pw, score_state_pw,score_state_ps]+value_ps_optS[tokens_pw, score_state_pw,score_state_ps])\n",
    "        iteration_round_zsgtwoplayers[tokens_pw,score_state_pw,score_state_ps] = round_index_zsgtwoplayers + 1\n",
    "        #print('optimize A and B iteratively in time={} seconds'.format(time.time()-t_opt_twoplayers_begin))\n",
    "    \n",
    "    #### finish a column        \n",
    "    #if (score_state_pb%20==0 or score_state_pb==2):\n",
    "    #    print('#### score_state_pb={}, time={}'.format(score_state_pb, time.time()-t_scoreloop_begin))\n",
    "\n",
    "## computation is done\n",
    "t2 = time.time()\n",
    "print('solve_zsg_opt_{}e{}_fix_{}e{} in {} seconds'.format(name_ps,epsilon_ps, name_pw, epsilon_pw, t2-t1))\n",
    "print('t_policy_evaluation  = {} seconds'.format(t_policy_evaluation))\n",
    "print('t_policy_improvement = {} seconds'.format(t_policy_improvement))\n",
    "print('t_other = {} seconds'.format(t_other)) "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "playerID = 10\n",
    "name_pw = 'player{}'.format(playerID)\n",
    "name_ps = 'player{}'.format(playerID)\n",
    "epsilon_pw = 2\n",
    "epsilon_ps = 1\n",
    "dp_policy_folder = 'result'\n",
    "postfix=''\n",
    "gpu_device=None\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_player10e2_S_player10e1_optboth\n",
      "player W is player10 e2 and player S is player10 e1\n",
      "optimize both players\n",
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_custom_no_tokens/player10_e2_gaussin_prob_grid_custom_no_tokens.pkl\n",
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_custom_no_tokens/player10_e1_gaussin_prob_grid_custom_no_tokens.pkl\n",
      "load_pickle from ./data_parameter/player_gaussin_fit/grid_custom_tokens/t_gaussin_prob_grid_custom_tokens.pkl\n",
      "load weaker player policy result/singlegame_player10_e2_turn_tokens.pkl\n",
      "load stronger player policy result/singlegame_player10_e1_turn_tokens.pkl\n",
      "stronger state: 2 time: 0.0\n",
      "stronger state: 3 time: 20.623115301132202\n",
      "stronger state: 4 time: 43.61317253112793\n",
      "stronger state: 5 time: 64.5770411491394\n"
     ]
    }
   ],
   "source": [
    "solve_zsg_optboth(name_pw, name_ps, epsilon_pw, epsilon_ps, data_parameter_dir=fb.data_parameter_dir, dp_policy_folder='result', result_dir='result', postfix='', gpu_device=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "darts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
